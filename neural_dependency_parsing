{"cells":[{"cell_type":"markdown","metadata":{"id":"BZlPHdsbYXlm"},"source":["# Natural Language Processing project II\n","\n","\n","Bresolin Paolo: 2060672\n","\n","Casarin Marco: 2044727"]},{"cell_type":"markdown","metadata":{"id":"9hjDhgyJSi8K"},"source":["# Dataset analysis\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJ1BWLgFy8VY","outputId":"e3615ad4-eecc-4016-ab00-91c6a5b97caf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Collecting aiohttp (from datasets)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 multiprocess-0.70.14 xxhash-3.2.0 yarl-1.9.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting conllu\n","  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n","Installing collected packages: conllu\n","Successfully installed conllu-4.5.3\n"]}],"source":["!pip install datasets  # huggingface library with dataset\n","!pip install conllu    # aux library for processing CoNLL-U format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VXRFZL9zCIS"},"outputs":[],"source":["import time\n","import random\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from functools import partial\n","from datasets import load_dataset\n","import matplotlib.pyplot as plt\n","from tabulate import tabulate"]},{"cell_type":"markdown","metadata":{"id":"pE8QGmYbZZdN"},"source":["## Load Dataset"]},{"cell_type":"markdown","metadata":{"id":"3oU4iXpHcZuS"},"source":["We have decided to load the $en\\_lines$ sub-dataset because:\n","1. The language is english, so it is easier to compare later with state of the art approaches;\n","2. It is the same sub-dataset used in the second lab session, therefore we can compare with lab results to check if our approach makes sense;\n","3. It has a fairly high number of sentences (also not too many, some other sub-datasets have too many data);\n","4. Other groups of students choose the same dataset, therefore we have a way of comparing our results with them.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":362,"referenced_widgets":["14ba408fd73347c798900adad2cadec0","cfbd3d87ba574d6e8d5b6d6f2b332ace","18e72b79dcc14bc2893f64592c7fd33e","7bf65a7c07ab4113b3fb5655f451ed0d","38482f6f1f4f4d93b7e3bec4789b5800","a252976311234ff19b043dc0bbb0c4a1","8c36251b9f594243bb08c45a7d87041f","b4242b2725084a259cb9d9ef5942584c","d585c7e01b534a49b8335291c3590f01","68110a9b63c94b40890d1c614aab9bf3","7a191cfc0d7e444d8a88c8a971a5b32c","f931f5d677584f77a44d8721631492f7","fe1d8424b41c4587b0b1c3c45725c342","920b8530633a4680a7df2890243c8df6","b57f9384adbc48988c2aeb5af4a8ebd7","29e1e600e46c4be38a2b95fe5b47b281","4d0a3a19930f4aafb9544d42c31752fc","732b5bad45cb4fbc93ad253f7fc91909","c13a01c8d2324dfc92eca91dce080fc9","19c3126ecda7493ca07196bfba1833e2","4b5969dbc7e74114a8f4a1352b952181","bc7c2ee676b24270bc8e95732aff0672","685ddb6b91c543a9995afb32593ddf61","824992f915b64136ab6326d6b3c99903","a84b750fdb8b4f9e99f1ebf78d54416d","505fdd1839e84b74bbc2eaba57500770","fd4a68745b374009a680acb32d27ed99","3c733e8de923424b959b463ce29ed709","59bfa05904a64e7cac8eb90502640af0","0307f6e57a9146499ca236f190aafeed","6ff3ec04ba74432ab24d6f8680b1440b","e7f78c3100ac4578bc8f32067e0e2af5","9ca5273015204a13ae9ba318c52bab4c","0dbda08363434b55a96616d27100bd93","285a0d13048b4d77bae402ba8a2dc563","26438e5ebb98469994a92374fd36aab0","256d29fe5f7144a08eed6d055191e10a","83925a91960b426285c3c1ee8aa063e1","19d43521019640a29b9e4c6293c66181","eea5ab2cfe1f4965b5c8f836b32d156d","eda6de9d06a74201b1e84cd4cafa4ecc","0c6a11c305234d81bf1f75b4491e006b","fb09c5f4d53c4310b6c1c8173d8134ca","07543e62ae0a499ab78f2f8eb61d4b14","54dd851a77e049c482c13ad6b3ce9b07","c560ac77117d4bcb897d74a73fac7ca0","1c8f88849cbe4c66a73c429e74d7b926","e302ba7cd1474db68fb58ad2e017c080","2ba48e2c142743c58d79e18352a485c3","5040b07cd9d2499bbaf273b510f11e14","83158d426e9a42579aaaaa09edfd3a9a","052b0e2983274ac78304a3f2b5cd6b5b","be636712a86741e8b338194088e63c54","14a1d6d68c464e2fb88062e9b7a56053","d4390fcaa82f4aaf937aa99efe06723c","d8de05240f41430887f13ac382b82530","983bac7fdb6649989e87082386c71dd4","a7ae27f9601d43b5bb84c1bafe35c6df","d02f31eb011a4ca1a8866317ae7f5573","498dcca95d6546159edc778bb19cd6e4","3767b43f519b4deb9ee52ac9da20e5de","98deb10113324a7788a30bd202fd6f6f","6fc36ebfc1024c27b9a3f30b4279c38e","7205ba93fd78485e97b6f56e574d9372","38f1022fcf934886ad7559f615a753ed","5fd09b4c763d4e529949562476b158e4","6761aaad23cd4f24822a398d2bc88305","03d8a555b6864475b5c060895400e104","928d1a9b3cf54b7787e97888cfffd569","b660ca28332e42c59bdabb3b9ec2e255","af6596169e4c4344ba8720608d58587a","8040c7e487d04f1a9d8974c39512942b","f0c3cc0c86e2411290d254cdebd8fbc8","8f04eec2d3bc40299a2fc8decd40a32c","358140b31ac14062a873c28cb31a61e0","62a05d75716a4c009e7d94d536338727","227c1ca2f18845dbb7acdcada55d93cf","693770d6c6e34bb591da8925df166ebd","5a7eec0a45904c84aaf6d0d0ae32a877","3928b038fa534debb825879ff57caf16","3756a5dcf5704ba79f2220b6ca700133","56e7c1f2abda4e2e9d3b94308570ac77","600d7f906d764ec082188e015c9064bd","9585d158af3b47a88a96547717b26f01","c12ec7a87f794ad2b3c980a5b0fffd24","9cf9bebd07194ec2aefabb95ce41339e","69a52a901c8140eea8555fe96ef437b8","99363db37f2041439c18aae8835b6e25","47dc925aec47429d94b284e0e112f252","3e29c4567d51493a9de39e772cf8a6f8","09eae29952b44137a63746d794cbe822","135f6b19d52d489692833bb6c1e2ef2f","59f527ac29fe44f991f21771f4e43054","52137e8c09004b09b01076b54f68dd4b","f107f7da4ca14a109644a7066d1666da","962ad4e8f8504344aa0e5d116ae486d8","bc048901ee5642f9aa2d0f9d626c70d0","d8dffe19768f4633bd91d6dad299dd4c","66b8045f044646478addbe77063659c0","9a804d137d8e4655b3127811d3493843","27b050ae62954951a255b6f591f79fd9","cec4cdff289a41c380ec321776ea536f","8d98a759d86845e2a3e51d936cf54f88","2e64f30bba404033bd17963d518576df","fb5034991fc0402c8a942bf3d28a6c0c","2d02ac1b302c46f89aadbe3ca34aaaec","e9d21ad92603471f8704fbacf421fa9c","bf8c170c7a0749c5a3ed7c5a2fe30030","65e1760ea3f841498823a77047c1149b","bf88eb61be0548f989dc8e550b186592","77bbd647de0e4bc9b4f0f93a26b6a1a5","caf8bac41f9648ae9fade863552a44be","2e725f275e9d48c5b49d7fb0217022d4","aef0d36b18884148ba428eebed3ca79b","095e382783f74b5faf87f4bc0bca455a","691884c3d355441b9d87fc3cea340e32","6807029f06a544f1bf5f17f2967054b3","a35a24b3fc2a45b89eeb860df05743b1","d73f7e8d4ac1430cad2412b02eae6b98","ba1688bbe13249bb9ec7040ffdfedf37","a2c7f1474aeb4c0c89382e45eafca4dd"]},"id":"Hr9rhTe7ZZEV","outputId":"9583e512-6d77-4d22-a6d8-1808df78584a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/87.8k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ba408fd73347c798900adad2cadec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading metadata:   0%|          | 0.00/2.33M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f931f5d677584f77a44d8721631492f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/191k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"685ddb6b91c543a9995afb32593ddf61"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset universal_dependencies/en_lines to /root/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dbda08363434b55a96616d27100bd93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/580k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54dd851a77e049c482c13ad6b3ce9b07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/199k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8de05240f41430887f13ac382b82530"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/181k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6761aaad23cd4f24822a398d2bc88305"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"693770d6c6e34bb591da8925df166ebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/3176 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47dc925aec47429d94b284e0e112f252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/1032 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a804d137d8e4655b3127811d3493843"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/1035 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77bbd647de0e4bc9b4f0f93a26b6a1a5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset universal_dependencies downloaded and prepared to /root/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7. Subsequent calls will reuse this data.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset universal_dependencies (/root/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n","WARNING:datasets.builder:Found cached dataset universal_dependencies (/root/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n"]}],"source":["# the training set has already been loaded, then load also development set and test set\n","dev_dataset = load_dataset(\"universal_dependencies\", \"en_lines\", split=\"validation\")\n","test_dataset = load_dataset(\"universal_dependencies\", \"en_lines\", split=\"test\")\n","train_dataset = load_dataset(\"universal_dependencies\", \"en_lines\", split=\"train\")"]},{"cell_type":"markdown","metadata":{"id":"Oqif56jzclwl"},"source":["## Some information about the dataset"]},{"cell_type":"markdown","metadata":{"id":"S90WF1IhdOD6"},"source":["Dataset sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JgB1TR1kaWg9","outputId":"59a5bf0a-d867-4e82-fd86-97defde06880"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training dataset size:  3176\n","Development dataset size:  1032\n","Test dataset size:  1035\n"]}],"source":["# size of train, dev, and test datasets\n","print(\"Training dataset size: \", len(train_dataset))\n","print(\"Development dataset size: \", len(dev_dataset))\n","print(\"Test dataset size: \", len(test_dataset))"]},{"cell_type":"markdown","metadata":{"id":"CZ6KhxjRdRC0"},"source":["Sentences length distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":459},"id":"buBQnr7mdUnL","outputId":"7bcb77e6-897a-4686-dbae-26ff15e97a2a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x400 with 3 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# compute sentences length\n","train_len_distr = [len(sentence) for sentence in train_dataset[\"text\"]]\n","dev_len_distr = [len(sentence) for sentence in dev_dataset[\"text\"]]\n","test_len_distr = [len(sentence) for sentence in test_dataset[\"text\"]]\n","\n","\n","# define some parameters and plot the length distributions\n","bins = 100\n","fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n","\n","lengths = [train_len_distr, dev_len_distr, test_len_distr]\n","titles = [\"Training set\", \"Development set\", \"Test set\"]\n","colors = [\"b\", \"r\", \"g\"]\n","\n","for i in range(len(axs)):\n","    axs[i].hist(lengths[i], bins=bins, color=colors[i])\n","    axs[i].set_title(titles[i])\n","    axs[i].set_xlabel(\"Sentences length\")\n","    axs[i].set_xlim([0, 500])\n","    axs[i].set_ylim([0, 180])\n","\n","fig.text(\n","    0.5,\n","    1,\n","    \"Sentences length distribution\",\n","    ha=\"center\",\n","    rotation=\"horizontal\",\n","    fontsize=22,\n",")\n","fig.text(0.04, 0.5, \"Frequency\", va=\"center\", rotation=\"vertical\")\n","\n","plt.subplots_adjust(wspace=0.4)\n","\n","# plot the figure\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"wRxLkgQtdQHH"},"source":["As one can see from the plots, the length distributions of the sentences in the train, development and test set are similar. Most of the sentences have a length in [5,150] characters as expected. In our everyday life we do not have longer sentences in general.\n","\n","For the sake of curiosity, let's compute also the shortest and the longest sentence in the three datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WIBWHfPHSuax","outputId":"539de8d8-dfb4-421a-e11d-93095ecf7e89"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training dataset    -->  shortest:4      longhest:494\n","Development dataset -->  shortest:5      longhest:459\n","Test dataset        -->  shortest:3      longhest:356\n"]}],"source":["print(\n","    f\"{'Training dataset':20}{'-->':5}{'shortest:'}{min(lengths[0])}{'longhest:':>15}{max(lengths[0])}\"\n",")\n","print(\n","    f\"{'Development dataset':20}{'-->':5}{'shortest:'}{min(lengths[1])}{'longhest:':>15}{max(lengths[1])}\"\n",")\n","print(\n","    f\"{'Test dataset':20}{'-->':5}{'shortest:'}{min(lengths[2])}{'longhest:':>15}{max(lengths[2])}\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"zwEBLvlrYnUi"},"source":["## Remove non projective trees and related sentences\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1_XelUKQYv9K"},"source":["In this section, all the non-projective trees are removed. Then, for the sake of curiosity, let us compute the length of the dataset to see how many instances have been removed. The _is_projective_ function is taken from 2nd lab session. We tried to desing a more efficient function, however we got the same asinthotical complexity, so we kept the original one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-EVepZJY8Oj"},"outputs":[],"source":["# naive function to check if a tree is projective.\n","def is_projective(tree):\n","    for i in range(len(tree)):\n","        if tree[i] == -1:\n","            continue\n","        left = min(i, tree[i])\n","        right = max(i, tree[i])\n","\n","        for j in range(0, left):\n","            if tree[j] > left and tree[j] < right:\n","                return False\n","        for j in range(left + 1, right):\n","            if tree[j] < left or tree[j] > right:\n","                return False\n","        for j in range(right + 1, len(tree)):\n","            if tree[j] > left and tree[j] < right:\n","                return False\n","\n","    return True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"prZrDNu1Y7dr"},"outputs":[],"source":["# keep only projective tree instances\n","train_dataset_proj = [\n","    sample\n","    for sample in train_dataset\n","    if is_projective([-1] + [int(head) for head in sample[\"head\"]])\n","]\n","dev_dataset_proj = [\n","    sample\n","    for sample in dev_dataset\n","    if is_projective([-1] + [int(head) for head in sample[\"head\"]])\n","]\n","test_dataset_proj = [\n","    sample\n","    for sample in test_dataset\n","    if is_projective([-1] + [int(head) for head in sample[\"head\"]])\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0MxUl7qJZysW","outputId":"1afeaa15-2345-423d-db19-5bc928531263"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training dataset size:  2922\n","Development dataset size:  930\n","Test dataset size:  968\n"]}],"source":["# print sizes of new datasets\n","print(\"Training dataset size: \", len(train_dataset_proj))\n","print(\"Development dataset size: \", len(dev_dataset_proj))\n","print(\"Test dataset size: \", len(test_dataset_proj))"]},{"cell_type":"markdown","metadata":{"id":"eHumeYrpacPB"},"source":["Less than 10% of the sentences have been lost due to non-projective trees.\n","\n","As a note, looking also at the above percentage, we want to underline that we have decided not to remove the non-projective sentences from all the three datasets (training set, validation set and test set). The decision has been taken after a discussion involving several ideas. We know that removing non-projective sentences from the test set would affect positively performances by increasing the UAS score. However, in our opinion, since the real world contains both projective and non-projective sentences, it does not make much sense to build a model that works only on projective sentences. For this reason, we also trained our models using non-projective sentences. It is true that, since the Parser only works for projective sentences, we cannot expect good performances with non-projective sentences, but training the models in a set containing also the latters allows the models to reduce the errors when parsing them. Obviously, this makes the models lose some performances when dealing with projective sentences. In conclusion, we can say that the choise has been taken in order to have models that can more realistically deal with unseen sentences."]},{"cell_type":"markdown","metadata":{"id":"Z9p3oFwpyphd"},"source":["# Arc-eager parser"]},{"cell_type":"markdown","metadata":{"id":"1Rsu48yUt-CL"},"source":["A configuration of the arc-eager parser is a triple of the form $(σ, β, A)$ where:\n","\n","* $\\sigma$ is the stack\n","* $\\beta$ is the buffer\n","* $A$ is the set of arcs constructed so far\n","\n","Let:\n","\n","* $\\beta_i$, $i\\geq1$ the $i$-th token in the buffer\n","* $\\sigma_i$, $i\\geq1$ the $i$-th token in the stack\n","\n","for the $i$-th configuration.\n","\n","The arc-eager parser can perform four types of actions (transitions):\n","\n","* **left-arc** (LA): create the arc $(\\beta_1 → \\sigma_1)$ and remove $\\sigma_1$ from the stack. The **preconditions** are: $\\sigma_1$ is not the ROOT and $\\sigma_1$ does not have already an head;\n","\n","* **right-arc** (RA): create the arc $(\\sigma_1 → \\beta_1)$ and push $\\beta_1$ to the stack;\n","\n","* **reduce** (RE): remove $\\sigma_1$ from the stack. The **precondition** is: $\\sigma_1$ must have a head;\n","\n","* **shift** (SH): remove $\\beta_1$ from the buffer and push it to the stack.\n","\n","\n","Let $w = w_0 w_1 \\cdots w_{n}$ be the input sentence, with $w_0$ the special symbol `<ROOT>`.\n","Stack and buffer are implemented as lists of integers, where `j` represents word $w_j$.  Top-most stack token is at the right-end of the list; first buffer token is at the left-end of the list.\n","Set $A$ is implemented as an array `arcs` of size $n+1$ such that if arc $(w_i \\rightarrow w_j)$ is in $A$ then `arcs[j]=i`, and if $w_j$ is still missing its head node in the tree under construction, then `arcs[j]=-1`. We always have `arcs[0]=-1`.  We use this representation also for complete dependency trees.\n","\n","The spurious ambiguities that can happen is between SH and RE operations. The greedy choice to solve the ambiguity is to give precedence to SH operation."]},{"cell_type":"markdown","metadata":{"id":"JtBjAcRvfNSv"},"source":["## Arc-eager class\n","\n","In this section, the parser class in defined, therefore the initialization of the stack and the buffer, transition operations and some other utility methods."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dk0DYnrpypBn"},"outputs":[],"source":["# NOTE: here only the transition methods are implemented\n","# the preconditions will be checked later with the boolean methods\n","\n","\n","class ArcEager:\n","    def __init__(self, sentence):\n","        self.sentence = sentence\n","        self.buffer = [i for i in range(len(self.sentence))]  # initialize buffer\n","        self.stack = []  # initialize empty stack\n","        self.arcs = [-1 for _ in range(len(self.sentence))]  # initialize arcs\n","\n","        # initialization of the stack with one SH operations\n","        # only the <ROOT> in the stack\n","        self.shift()\n","\n","    # left arc transition\n","    def left_arc(self):\n","        b1 = self.buffer[0]  # first element in the buffer\n","        o1 = self.stack.pop()  # topmost element in the stack\n","        self.arcs[o1] = b1  # create the arc o1 -> b1\n","\n","    # right arc transition\n","    def right_arc(self):\n","        b1 = self.buffer[0]\n","        o1 = self.stack[-1]\n","        self.arcs[b1] = o1\n","        self.buffer = self.buffer[1:]\n","        self.stack.append(b1)\n","\n","    # reduce transition\n","    def reduce(self):\n","        _ = self.stack.pop()\n","\n","    # shift transition\n","    def shift(self):\n","        b1 = self.buffer[0]\n","        self.buffer = self.buffer[1:]\n","        self.stack.append(b1)\n","\n","    # print configuration (for debug mainly)\n","    def print_configuration(self):\n","        s = [self.sentence[i] for i in self.stack]\n","        b = [self.sentence[i] for i in self.buffer]\n","        print(s, b)  # print stack and buffer\n","        print(self.arcs)  # print arcs\n","\n","    # check if parsing in completed\n","    def is_tree_final(self):\n","        return len(self.buffer) == 0  # parsing completed when buffer is empty"]},{"cell_type":"markdown","metadata":{"id":"DFp41SZ1ga9_"},"source":["## Oracle class\n","\n","In this section, the oracle class in defined. To access whether a transition is gold, the gold tree and the precoditions are checked."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzsqgSq3gsF3"},"outputs":[],"source":["class Oracle:\n","    def __init__(self, parser, gold_tree):\n","        self.parser = parser\n","        self.gold = gold_tree\n","\n","    # is left arc gold?\n","    def is_left_arc_gold(self):\n","        # get the two elems the arc should be built on\n","        b1 = self.parser.buffer[0]  # first element in the buffer\n","        o1 = self.parser.stack[\n","            len(self.parser.stack) - 1\n","        ]  # topmost element in the stack\n","\n","        # check preconditions\n","        if o1 == -1:\n","            return False\n","        if self.parser.arcs[o1] != -1:\n","            return False\n","\n","        # check if gold move by looking the gold tree\n","        if self.gold[o1] != b1:\n","            return False\n","        else:\n","            return True\n","\n","    # is right arc gold?\n","    def is_right_arc_gold(self):\n","        b1 = self.parser.buffer[0]\n","        o1 = self.parser.stack[len(self.parser.stack) - 1]\n","\n","        # no preconditions\n","        # check if gold move\n","        if self.gold[b1] != o1:\n","            return False\n","        else:\n","            return True\n","\n","    # is reduce gold?\n","    def is_reduce_gold(self):\n","        # check precondition\n","        o1 = self.parser.stack[len(self.parser.stack) - 1]\n","        if self.parser.arcs[o1] == -1:\n","            return False\n","\n","        # check if exist k < o1 s.t. exist (k,b1) or (b1,k) in the gold tree\n","        b1 = self.parser.buffer[0]\n","        if self.gold[b1] < o1:\n","            return True\n","        else:\n","            return b1 in self.gold[:o1]\n","\n","        # shift is always gold"]},{"cell_type":"markdown","metadata":{"id":"eD2mgyECpYaT"},"source":["## Some parsing utilities\n","\n","Let's define some utility functions to parse a sentence and the entire dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fN6h2uaipX3G"},"outputs":[],"source":["# Function to parse a single sentence. It takes an instance of the oracle class as input.\n","def parse_sentence(oracle, show_conf=False):\n","    if show_conf:\n","        print(\"Parsing steps:\")\n","    iteration = 0  # keep track of the number of iterations\n","\n","    # until the tree is final, apply the oracle move\n","    while not oracle.parser.is_tree_final():\n","        if show_conf:\n","            print(\"Iteration:\", iteration)\n","\n","        # if LA is gold\n","        if oracle.is_left_arc_gold():\n","            oracle.parser.left_arc()\n","            if show_conf:\n","                oracle.parser.print_configuration()\n","                print(\"Transition: LA\", end=\"\\n\\n\")\n","\n","        # elif RA is gold\n","        elif oracle.is_right_arc_gold():\n","            oracle.parser.right_arc()\n","            if show_conf:\n","                oracle.parser.print_configuration()\n","                print(\"Transition: RA\", end=\"\\n\\n\")\n","\n","        # elif RE is gold\n","        elif oracle.is_reduce_gold():\n","            oracle.parser.reduce()\n","            if show_conf:\n","                oracle.parser.print_configuration()\n","                print(\"Transition: RE\", end=\"\\n\\n\")\n","\n","        # else shift (always gold)\n","        else:\n","            oracle.parser.shift()\n","            if show_conf:\n","                oracle.parser.print_configuration()\n","                print(\"Transition: SH\", end=\"\\n\\n\")\n","\n","        iteration = iteration + 1\n","    return oracle.parser.arcs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYVIXWd9r1No"},"outputs":[],"source":["# Function to parse a dataset.\n","# Takes as input a dataset of sentences.\n","# The output is True if all sentences are correctly parsed, False otherwise.\n","def parse_dataset(dataset):\n","    for sample in dataset:\n","        # add the <ROOT>\n","        sentence = [\"<ROOT>\"] + sample[\"tokens\"]\n","        gold_tree = sample[\"head\"]\n","        gold_tree = [-1] + [int(key) for key in gold_tree]\n","\n","        # initialize parser and oracle\n","        parser = ArcEager(sentence)\n","        oracle = Oracle(parser, gold_tree)\n","\n","        # parse sentence\n","        parse_tree = parse_sentence(oracle)\n","\n","        # if parsing is not correct, return False\n","        if parse_tree != gold_tree:\n","            return False\n","    return True"]},{"cell_type":"markdown","metadata":{"id":"GuV4v5F4qpp1"},"source":["## Test on a sentence\n","\n","Now let's define by hand a sentence and test the parser on it."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kW4WfMUnqaZg","outputId":"34f0df17-bd59-482c-f27a-022c9756917b"},"outputs":[{"output_type":"stream","name":"stdout","text":["['<ROOT>'] ['He', 'wrote', 'her', 'a', 'letter', '.']\n","[-1, -1, -1, -1, -1, -1, -1]\n","Parsing steps:\n","Iteration: 0\n","['<ROOT>', 'He'] ['wrote', 'her', 'a', 'letter', '.']\n","[-1, -1, -1, -1, -1, -1, -1]\n","Transition: SH\n","\n","Iteration: 1\n","['<ROOT>'] ['wrote', 'her', 'a', 'letter', '.']\n","[-1, 2, -1, -1, -1, -1, -1]\n","Transition: LA\n","\n","Iteration: 2\n","['<ROOT>', 'wrote'] ['her', 'a', 'letter', '.']\n","[-1, 2, 0, -1, -1, -1, -1]\n","Transition: RA\n","\n","Iteration: 3\n","['<ROOT>', 'wrote', 'her'] ['a', 'letter', '.']\n","[-1, 2, 0, 2, -1, -1, -1]\n","Transition: RA\n","\n","Iteration: 4\n","['<ROOT>', 'wrote', 'her', 'a'] ['letter', '.']\n","[-1, 2, 0, 2, -1, -1, -1]\n","Transition: SH\n","\n","Iteration: 5\n","['<ROOT>', 'wrote', 'her'] ['letter', '.']\n","[-1, 2, 0, 2, 5, -1, -1]\n","Transition: LA\n","\n","Iteration: 6\n","['<ROOT>', 'wrote'] ['letter', '.']\n","[-1, 2, 0, 2, 5, -1, -1]\n","Transition: RE\n","\n","Iteration: 7\n","['<ROOT>', 'wrote', 'letter'] ['.']\n","[-1, 2, 0, 2, 5, 2, -1]\n","Transition: RA\n","\n","Iteration: 8\n","['<ROOT>', 'wrote'] ['.']\n","[-1, 2, 0, 2, 5, 2, -1]\n","Transition: RE\n","\n","Iteration: 9\n","['<ROOT>', 'wrote', '.'] []\n","[-1, 2, 0, 2, 5, 2, 2]\n","Transition: RA\n","\n","Is tree correct? True\n"]}],"source":["# define the sentence and the gold tree (projective)\n","sentence = [\"<ROOT>\", \"He\", \"wrote\", \"her\", \"a\", \"letter\", \".\"]\n","gold = [-1, 2, 0, 2, 5, 2, 2]\n","\n","# initialize parser and oracle\n","parser = ArcEager(sentence)\n","oracle = Oracle(parser, gold)\n","\n","# print initial configuration\n","parser.print_configuration()\n","\n","# parse the sentence\n","arcs = parse_sentence(oracle, show_conf=True)\n","\n","# check the parsing is correct\n","print(\"Is tree correct?\", oracle.gold == arcs)"]},{"cell_type":"markdown","metadata":{"id":"xEEEdP7qkP6u"},"source":["## Test on the entire dataset\n","\n","In order to really be sure that the implementation of the parser and the oracle is correct, we test it on the whole dataset (but only on instances with projective trees)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIOHl1yskim7","outputId":"4c70a490-03a8-4d7f-e9c4-89c04c0b7418"},"outputs":[{"output_type":"stream","name":"stdout","text":["Check parsing on training set:     True\n","Check parsing on development set:  True\n","Check parsing on test set:         True\n"]}],"source":["# test parsing on the train, dev, and test set\n","print(f\"{'Check parsing on training set:':35}{parse_dataset(train_dataset_proj)}\")\n","print(f\"{'Check parsing on development set:':35}{parse_dataset(dev_dataset_proj)}\")\n","print(f\"{'Check parsing on test set:':35}{parse_dataset(test_dataset_proj)}\")"]},{"cell_type":"markdown","source":["# Baseline Bi-LSTM model"],"metadata":{"id":"KWNVmxS0td2T"}},{"cell_type":"markdown","source":["In the following cells the preprocessing, definition and training steps for the baseline Bi-LSTM model are performed"],"metadata":{"id":"NmZY8jyethmH"}},{"cell_type":"markdown","metadata":{"id":"1VAmB2M_FGCi"},"source":["## Baseline Bi-LSTM model - PREPROCESSING"]},{"cell_type":"markdown","metadata":{"id":"ENYJ5sUvc5i5"},"source":["### Pre-processing utils\n","- The $create\\_token\\_indices$ function creates a vocabulary $vocab$ containing all tokens in $dataset$ (given as input argument) appearing at least $threshold$ (given as input argument, default value 3) times. $vocab[token]$ is the index assigned to $token$;\n","\n","- the $process\\_sample$ function is used to process a single sample in our dataset and extract the needed training information from it.\n","Given a sentence, we use our oracle to compute the canonical path followed to extract the gold tree. We then pair each configuration to the golden transition selected by the oracle.\n","Because of the structure of Arc-Eager parser, we encode a $configuration$ with only two words: $\\sigma_1$ and $\\beta_1$ (i.e. the topmost element on the stack and the first buffer element, respectiveley);\n","\n","- $prepare\\_batch$ function pre-processes a batch of samples $batch\\_data$ using as indices of tokens the ones contained in the vocabulary $tokens\\_indices\\_voc$. The pre-processing is done by applying function $process\\_sample$ to each sample in $batch\\_data$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZm-g2DFd73Y"},"outputs":[],"source":["# The function returns a dictionary containing the vocabulary embedding indices for the tokens in dataset,\n","# i.e. an index is associated to each token in dataset.\n","# threshold is the minimum number of appearance for a token in dataset to be included in the dictionary.\n","def create_token_indices(dataset, threshold=3):\n","    # dic has the tokens as keys. Given a token, dic[token] is the number of occurrences of token in dataset\n","    dic = {}\n","    for sample in dataset:\n","        for token in sample[\"tokens\"]:\n","            if token in dic:\n","                dic[token] += 1\n","            else:\n","                dic[token] = 1\n","\n","    # vocab[\"token\"] is an integer representing the index of token \"token\"\n","    vocab = {}\n","    # indices for some special tokens\n","    vocab[\"<pad>\"] = 0\n","    vocab[\"<ROOT>\"] = 1\n","    vocab[\"<unk>\"] = 2\n","\n","    next_ind = 3\n","    for token in dic.keys():\n","        if dic[token] >= threshold:\n","            vocab[token] = next_ind\n","            next_ind += 1\n","\n","    return vocab\n","\n","\n","# Extracts training information from one sample if get_gold_path is True, otherwise does only a simple pre-process.\n","# sample is a sample of our dataset.\n","def process_sample(tokens_indices_voc, sample, get_gold_path=False):\n","    # add the root token to the sentence and its head (-1) to the gold head list\n","    sentence = [\"<ROOT>\"] + sample[\"tokens\"]\n","    gold = [-1] + [int(i) for i in sample[\"head\"]]\n","\n","    # sentence representation with each token represented by its index in the vocabulary\n","    sentence_repr = [\n","        tokens_indices_voc[token]\n","        if token in tokens_indices_voc\n","        else tokens_indices_voc[\"<unk>\"]\n","        for token in sentence\n","    ]\n","\n","    # gold_configurations and gold_transitions are parallel arrays whose elements refer to parsing steps\n","    # gold_configurations[i] records configuration at step i, i.e. topmost stack token and first buffer token for current step\n","    gold_configurations = []\n","    # gold_transitions[i] contains oracle (canonical) transition for step i: 0 is left_arc, 1 right_arc, 2 reduce, 3 shift\n","    gold_transitions = []\n","\n","    # only for training\n","    if get_gold_path:\n","        parser = ArcEager(sentence)\n","        oracle = Oracle(parser, gold)\n","\n","        while not parser.is_tree_final():\n","            # save configuration\n","            configuration = [parser.stack[-1]]\n","            if len(parser.buffer) == 0:\n","                configuration.append(-1)\n","            else:\n","                configuration.append(parser.buffer[0])\n","            gold_configurations.append(configuration)\n","\n","            # save gold transition\n","            if oracle.is_left_arc_gold():\n","                gold_transitions.append(0)\n","                oracle.parser.left_arc()\n","            elif oracle.is_right_arc_gold():\n","                gold_transitions.append(1)\n","                oracle.parser.right_arc()\n","            elif oracle.is_reduce_gold():\n","                gold_transitions.append(2)\n","                oracle.parser.reduce()\n","            else:\n","                gold_transitions.append(3)\n","                oracle.parser.shift()\n","\n","    # sentence_repr is a list containing representations of tokens in sample\n","    # gold is a list containning the representation of the gold tree of sample\n","    # gold_configurations is a list containing gold configurations\n","    # gold_transitions is a list containing gold transitions\n","    return sentence_repr, gold_configurations, gold_transitions, gold\n","\n","\n","# This function is used to pre-process a batch of samples batch_data from the original dataset\n","# applying function process_sample to each sample in batch_data.\n","def prepare_batch(tokens_indices_voc, batch_data, get_gold_path=False):\n","    processed_batch_data = [\n","        process_sample(tokens_indices_voc, sample, get_gold_path=get_gold_path)\n","        for sample in batch_data\n","    ]\n","\n","    sentences_repr = []\n","    paths = []\n","    moves = []\n","    gold_trees = []\n","    for sample in processed_batch_data:\n","        sentences_repr.append(sample[0])\n","        paths.append(sample[1])\n","        moves.append(sample[2])\n","        gold_trees.append(sample[3])\n","\n","    # sentences_repr, paths, moves, gold_trees are parallel lists\n","    # element in position i of each of the above lists refers to the same sentence i\n","    return sentences_repr, paths, moves, gold_trees"]},{"cell_type":"markdown","metadata":{"id":"Ddwo8I983JYB"},"source":["### Datasets loading and pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BSY8aixm-Aj2"},"outputs":[],"source":["# set the number of samples per batch\n","BATCH_SIZE = 32\n","\n","# create the dictionary with token indices that is the embedding dictionary\n","tokens_indices = create_token_indices(train_dataset)\n","\n","# create dataloaders to batch each dataset and apply function prepare_batch to each batch\n","train_dataloader_base = torch.utils.data.DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    collate_fn=partial(prepare_batch, tokens_indices, get_gold_path=True),\n",")\n","validation_dataloader_base = torch.utils.data.DataLoader(\n","    dev_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    collate_fn=partial(prepare_batch, tokens_indices),\n",")\n","test_dataloader_base = torch.utils.data.DataLoader(\n","    test_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    collate_fn=partial(prepare_batch, tokens_indices),\n",")"]},{"cell_type":"markdown","metadata":{"id":"DnsNusqN5yqa"},"source":["Now everything is ready to create a BiLSTM-based model that extracts embeddings from configurations in order to then feed a classifier."]},{"cell_type":"markdown","metadata":{"id":"ilo0vK4GgfkH"},"source":["## Baseline Bi-LSTM model - DEFINITION\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2FrFU3nNO3BJ"},"source":["In this section we create the Bi-LSTM baseline model. It consists of Bi-LSTM layer(s) followed by feedforward layer(s). The Bi-LSTM part captures the information about the meaning and the context of the words and stores it in its hidden states variables. The feedforward layers combine this information to get the probability distribution over the transitions to predict the best one."]},{"cell_type":"markdown","metadata":{"id":"Ge9y44h9hEr1"},"source":["In the following cell some parameters of the model are defined."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUeZlwqFg-J5"},"outputs":[],"source":["EMBEDDING_SIZE = 200  # size of the tokens embedding\n","LSTM_SIZE = 200  # LSTM layer size\n","LSTM_LAYERS = 2  # number of LSTM layers\n","MLP_SIZE = 200  # multi layer perceptron size (feedforward part)\n","DROPOUT = 0.3  # droput layer (probability of dropping nodes)\n","EPOCHS = 30  # number of training epochs\n","LR = 0.005  # learning rate"]},{"cell_type":"markdown","metadata":{"id":"4GFgn_qhhY-J"},"source":["### Description of the baseline model"]},{"cell_type":"markdown","metadata":{"id":"_JPxS5EHn478"},"source":["Now we create the Bi-LSTM baseline model (the class) and all the methods needed for training and evaluating the Bi-LSTM baseline model.\n","\n","The model architecture consists of the cascade of two main parts: the Bi-LSTM block followed by a multi-layer perceptron (MLP) (that consists in the cascade of two feedforward layers). Apart from the parameters in the above cell, the important things are the following:\n","* the first feedforward layer must have input dimension of size `4*LSTM_SIZE`, because each configuration contains two elements, top most in the stack and first in the buffer, and the LSTM layer(s) are bi-directional (therefore the hidden state representation has size `2*LSTM_SIZE`);\n","* the second feedforward layer must have output dimension of size 4, since we need to get predictions that are the probability distribution over the 4 possible transitions (classifier with 4 labels).\n","\n","\n","The Bi-LSTM layers allow the model to learn information about the tokens and their meaning, also considering the context in which they appear. The multi-layer perceptron block, instead, allows to reason about the information gathered inside the hidden states of the Bi-LSTM block to predict the best transition."]},{"cell_type":"markdown","metadata":{"id":"HVVBjfx-Pcrs"},"source":["#### parse_step function: explanation\n","\n","The definition of this function is really important at inference time. Theoretically, given the scores predictions by the model, the parse step should take the highest score transition. However, some transitions are not feasible, such as shifting when the buffer is empty. Therefore, this function must implement some additional checks about the feasibility of the best transition and, if not feasible, go for the second or third best transition. The implementation of the function is not unique and it may affect the quality of the model. We tried to train both the models with different implementations of $parse\\_step$, trying to understand what were the changes and why they occurred. Eventually, we came up with the choice of the current implementation, because it is a good trade-off between a too peaky function and a too loose one. In this way, we guide a bit the model at inference time, but we also give it the opportunity to learn from mistakes at training time.\n","We also compared our results with the ones obtained by another group and we understood that they got slihgtly better results because their ${parse\\_step}$ function has less conditions than our, and so modifies less what predicted by the model, giving it the possibility of better learn from mistakes.\n","Since we know it is quite hard to understand our implementation from the code, we report a description of it in the next cell."]},{"cell_type":"markdown","metadata":{"id":"98qBJS7GFREY"},"source":["\n","\n","* If the best move is LA: we need to check if it is a feasible move, that is:\n","  * $\\sigma_1$ does not have already a head (*a token cannot have two heads*);\n","  * $\\sigma_1$ is not the ROOT (*the ROOT cannot be a depentent*).\n","\n","  If this is not the case, then all RA, RE and SH are theoretically feasible moves. So we need to check the second-best predicted move:\n","  * if the second-best move is RA: choose it;\n","  * if the second-best move is RE: we need to check if it is a feasible move:\n","    * if $\\sigma_1$ is not the ROOT, then do RE;\n","    * otherwise, at this point we can do theoretically both RA or SH, so we need to check the third-best predicted move. In this case we have $\\sigma_1$ that is the ROOT:\n","      * if the third-best move is RA: do RA;\n","      * if the third-best move is SH: do SH;\n","  * if the second-best move is SH: do SH.\n","* If the best move is RA: choose RA.\n","* If the best move is RE:\n","  * if $\\sigma_1$ is not the ROOT, then do RE;\n","  * if $\\sigma_1$ is the ROOT, then we can do theoretically both SH or RA, while LA is not possible (otherwise the ROOT becomes a dependent):\n","    * if the second-best move is RA: do RA;\n","    * else: do SH.\n","* If the best move is SH: do SH without any problem."]},{"cell_type":"markdown","metadata":{"id":"Mic6xNeLSQCW"},"source":["### Definition of the baseline model class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TW4gPzMzb1LR"},"outputs":[],"source":["class Net(nn.Module):\n","    # initialization of the model\n","    def __init__(self, device):\n","        super(Net, self).__init__()\n","        self.device = device\n","        self.embeddings = nn.Embedding(\n","            len(tokens_indices), EMBEDDING_SIZE, padding_idx=tokens_indices[\"<pad>\"]\n","        )\n","\n","        # Bi-LSTM layer\n","        self.lstm = nn.LSTM(\n","            EMBEDDING_SIZE,\n","            LSTM_SIZE,\n","            num_layers=LSTM_LAYERS,\n","            bidirectional=True,\n","            dropout=DROPOUT,\n","        )\n","\n","        # feedforward layer with softmax activation function to get probability distrubution\n","        # the input layer size must be equal to 4*LSTM_SIZE because each configuration\n","        # contains 2 elements (first element in the buffer and topmost element in the stack)\n","        # and the LSTM layer is bi-derectional therefore 2*2 = 4 times LSTM_SIZE needed as input dimension\n","        self.w1 = torch.nn.Linear(4 * LSTM_SIZE, MLP_SIZE, bias=True)\n","        self.activation = torch.nn.Tanh()\n","\n","        # the output layer size must be equal to 4 because we need to get as predictions\n","        # the probability over the 4 possible transitions of the ArcEager parser\n","        self.w2 = torch.nn.Linear(MLP_SIZE, 4, bias=True)\n","        self.softmax = torch.nn.Softmax(dim=-1)\n","\n","        # add a dropout layer that helps to avoid overfitting\n","        self.dropout = torch.nn.Dropout(DROPOUT)\n","\n","    # this function performs a feedforward pass in the model\n","    def forward(self, x, paths):\n","        # load the embeddings\n","        x = [self.dropout(self.embeddings(torch.tensor(i).to(self.device))) for i in x]\n","\n","        # pass through the Bi-LSTM layer(s)\n","        h = self.lstm_pass(x)\n","\n","        # from the hidden representation of the Bi-LSTM layer(s) get the input for MLP\n","        mlp_input = self.get_mlp_input(paths, h)\n","\n","        # feedforward pass trough the MLP to get the predictions\n","        out = self.mlp(mlp_input)\n","        return out\n","\n","    # pass through the LSTM layer\n","    def lstm_pass(self, x):\n","        x = torch.nn.utils.rnn.pack_sequence(x, enforce_sorted=False)\n","        h, (h_0, c_0) = self.lstm(x)\n","        h, h_sizes = torch.nn.utils.rnn.pad_packed_sequence(\n","            h\n","        )  # size h: (length_sentences, batch, output_hidden_units)\n","        return h\n","\n","    # prepare data for the MLP from LSTM hidden states\n","    def get_mlp_input(self, configurations, h):\n","        mlp_input = []\n","        zero_tensor = torch.zeros(2 * LSTM_SIZE, requires_grad=False).to(self.device)\n","        for i in range(len(configurations)):  # for every sentence in the batch\n","            for j in configurations[i]:  # for each configuration of a sentence\n","                mlp_input.append(\n","                    torch.cat(\n","                        [\n","                            zero_tensor if j[0] == -1 else h[j[0]][i],\n","                            zero_tensor if j[1] == -1 else h[j[1]][i],\n","                        ]\n","                    )\n","                )\n","        mlp_input = torch.stack(mlp_input).to(self.device)\n","        return mlp_input\n","\n","    # MLP block\n","    def mlp(self, x):\n","        return self.softmax(\n","            self.w2(self.dropout(self.activation(self.w1(self.dropout(x)))))\n","        )\n","\n","    # this function parses a sentence based on the prediction from the baseline model\n","    # at each step choose the transition with the highest score\n","    def infere(self, x):\n","        # initialize the parser\n","        parsers = [ArcEager(i) for i in x]\n","\n","        # get the embeddings\n","        x = [self.embeddings(torch.tensor(i).to(self.device)) for i in x]\n","\n","        # pass through the LSTM layer\n","        h = self.lstm_pass(x)\n","\n","        # until all the sentences are parsed\n","        while not self.parsed_all(parsers):\n","            # get the current configuration and predict the best transition\n","            configurations = self.get_configurations(parsers)\n","            mlp_input = self.get_mlp_input(configurations, h)\n","            mlp_out = self.mlp(mlp_input)\n","\n","            # perform the best transition\n","            self.parse_step(parsers, mlp_out)\n","\n","        # once alla sentences have beed parsed, return the corresponding trees\n","        return [parser.arcs for parser in parsers]\n","\n","    # function to get the current configuration of each parser\n","    def get_configurations(self, parsers):\n","        configurations = []\n","\n","        # for each parser\n","        for parser in parsers:\n","            # if parsing complete, return [-1,-1] config\n","            if parser.is_tree_final():\n","                conf = [-1, -1]\n","            else:\n","                # otherwise get the top most element in the stack\n","                conf = [parser.stack[len(parser.stack) - 1]]\n","                # and the first element in the stack\n","                # NOTE: buffer never empty here, otherwise tree is final\n","                conf.append(parser.buffer[0])\n","\n","            # append the configuration\n","            configurations.append([conf])\n","\n","        return configurations\n","\n","    # function to check if all the sentences have been fully parsed\n","    def parsed_all(self, parsers):\n","        for parser in parsers:\n","            if not parser.is_tree_final():\n","                return False\n","        return True\n","\n","    # In this function we select and perform the next move according to the scores obtained.\n","    # We need to be careful and select correct moves, e.g. don't do a shift if the buffer\n","    # is empty or a left arc if σ2 is the ROOT. For clarity sake we didn't implement\n","    # these checks in the parser so we must do them here. This renders the function quite ugly\n","    def parse_step(self, parsers, moves):\n","        # get the move with the highest score\n","        best_moves = torch.topk(moves, 3, dim=1).indices\n","        best_predicted_move = best_moves[:, 0]\n","        second_best_predicted_move = best_moves[:, 1]\n","        third_best_predicted_move = best_moves[:, 2]\n","\n","        for idx in range(len(parsers)):\n","            # if tree is final no need to do next move\n","            if parsers[idx].is_tree_final():\n","                continue\n","            else:\n","                # =========================\n","                # if best move is LEFT ARC\n","                # =========================\n","                if best_predicted_move[idx] == 0:\n","                    # NOTE: buffer for sure not empty ALWAYS\n","\n","                    # check sigma_1 is not the root and sigma_1 doeas not have a head already\n","                    if (\n","                        parsers[idx].stack[-1] != 0\n","                        and parsers[idx].arcs[parsers[idx].stack[-1]] == -1\n","                    ):\n","                        parsers[idx].left_arc()\n","\n","                    # theoretically I can do RA, RE or SH, check the one with highest prob\n","                    else:\n","                        # if second best is right arc\n","                        if second_best_predicted_move[idx] == 1:\n","                            parsers[idx].right_arc()\n","\n","                        # if second best is reduce\n","                        elif second_best_predicted_move[idx] == 2:\n","                            # if sigma1 not the root we can reduce since for sure it has already\n","                            # a head\n","                            if parsers[idx].stack[-1] != 0:\n","                                parsers[idx].reduce()\n","\n","                            # if sigma1 is the root\n","                            # at this point we can do both RA or SH\n","                            else:\n","                                # right-arc has higher score\n","                                if third_best_predicted_move[idx] == 1:\n","                                    parsers[idx].right_arc()\n","\n","                                # if shift has higher score\n","                                else:\n","                                    parsers[idx].shift()\n","\n","                        # if second best is shift do it, no problems\n","                        else:\n","                            parsers[idx].shift()\n","\n","                # =========================\n","                # if best move is RIGHT ARC\n","                # =========================\n","                elif best_predicted_move[idx] == 1:\n","                    parsers[idx].right_arc()\n","\n","                # =========================\n","                # if best move is REDUCE\n","                # =========================\n","                elif best_predicted_move[idx] == 2:\n","                    # if sigma1 not the ROOT then RE is ok\n","                    if parsers[idx].stack[-1] != 0:\n","                        parsers[idx].reduce()\n","\n","                    else:\n","                        # at this point we can do both RA or SH\n","\n","                        # if second best move right arc\n","                        if second_best_predicted_move[idx] == 1:\n","                            parsers[idx].right_arc()\n","\n","                        # if second best move is SH\n","                        else:\n","                            parsers[idx].shift()\n","\n","                # =========================\n","                # if best move is SHIFT\n","                # =========================\n","                else:\n","                    # for shift no problems\n","                    parsers[idx].shift()"]},{"cell_type":"markdown","metadata":{"id":"r3OJTK35pwCC"},"source":["## Baseline Bi-LSTM model - TRAINING"]},{"cell_type":"markdown","metadata":{"id":"tVR-wSuLWKf3"},"source":["In this section some training utilities are defined and then the baseline model is trained."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tp9ODMPRTdtu"},"outputs":[],"source":["# function to evaluate the performances of the model\n","def evaluate(gold, preds):\n","    # counting variables\n","    total = 0\n","    correct = 0\n","\n","    # for each tree\n","    for g, p in zip(gold, preds):\n","        # count how many arcs are correct\n","        for i in range(1, len(g)):\n","            total += 1\n","            if g[i] == p[i]:\n","                correct += 1\n","\n","    # return the fraction of correct arcs\n","    # that is the Unlabeled Accuracy Score (UAS)\n","    return correct / total\n","\n","\n","# function to perform the training procedure\n","def train(model, dataloader, criterion, optimizer):\n","    model.train()  # set the model to training mode\n","    total_loss = 0\n","    count = 0\n","\n","    # for each batch\n","    for batch in dataloader:\n","        optimizer.zero_grad()\n","        sentences, paths, moves, trees = batch\n","\n","        # get the predictions and compute the loss\n","        out = model(sentences, paths)\n","        labels = torch.tensor(sum(moves, [])).to(device)\n","        loss = criterion(out, labels)\n","\n","        count += 1\n","        total_loss += loss.item()\n","\n","        # apply optimizer step\n","        loss.backward()\n","        optimizer.step()\n","\n","    return total_loss / count\n","\n","\n","# function to test the model on a sub-dataset\n","def test_base(model, dataloader):\n","    model.eval()  # set the model to evaluation mode\n","    gold = []\n","    preds = []\n","\n","    # for each batch\n","    for batch in dataloader:\n","        sentences, paths, moves, trees = batch\n","\n","        # get the predictions\n","        with torch.no_grad():\n","            pred = model.infere(sentences)\n","\n","            gold += trees\n","            preds += pred\n","\n","    # evaluate the model with UAS score\n","    return evaluate(gold, preds)"]},{"cell_type":"markdown","metadata":{"id":"sjkSmlt7aBN7"},"source":["### Train the baseline model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ENPaU1s6Tj6P","outputId":"1f003945-8036-4b79-8269-758458c1174b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device available: cuda\n","Epoch:   0 | avg_train_loss: 1.021 | dev_uas: 0.623 |\n","Epoch:   1 | avg_train_loss: 0.929 | dev_uas: 0.680 |\n","Epoch:   2 | avg_train_loss: 0.904 | dev_uas: 0.691 |\n","Epoch:   3 | avg_train_loss: 0.888 | dev_uas: 0.720 |\n","Epoch:   4 | avg_train_loss: 0.877 | dev_uas: 0.731 |\n","Epoch:   5 | avg_train_loss: 0.869 | dev_uas: 0.729 |\n","Epoch:   6 | avg_train_loss: 0.862 | dev_uas: 0.734 |\n","Epoch:   7 | avg_train_loss: 0.859 | dev_uas: 0.747 |\n","Epoch:   8 | avg_train_loss: 0.854 | dev_uas: 0.744 |\n","Epoch:   9 | avg_train_loss: 0.852 | dev_uas: 0.749 |\n","Epoch:  10 | avg_train_loss: 0.849 | dev_uas: 0.757 |\n","Epoch:  11 | avg_train_loss: 0.846 | dev_uas: 0.751 |\n","Epoch:  12 | avg_train_loss: 0.845 | dev_uas: 0.751 |\n","Epoch:  13 | avg_train_loss: 0.844 | dev_uas: 0.757 |\n","Epoch:  14 | avg_train_loss: 0.843 | dev_uas: 0.753 |\n","Epoch:  15 | avg_train_loss: 0.842 | dev_uas: 0.757 |\n","Epoch:  16 | avg_train_loss: 0.842 | dev_uas: 0.759 |\n","Epoch:  17 | avg_train_loss: 0.840 | dev_uas: 0.765 |\n","Epoch:  18 | avg_train_loss: 0.840 | dev_uas: 0.759 |\n","Epoch:  19 | avg_train_loss: 0.839 | dev_uas: 0.759 |\n","Epoch:  20 | avg_train_loss: 0.838 | dev_uas: 0.758 |\n","Epoch:  21 | avg_train_loss: 0.838 | dev_uas: 0.761 |\n","Epoch:  22 | avg_train_loss: 0.837 | dev_uas: 0.764 |\n","Epoch:  23 | avg_train_loss: 0.838 | dev_uas: 0.761 |\n","Epoch:  24 | avg_train_loss: 0.837 | dev_uas: 0.761 |\n","Epoch:  25 | avg_train_loss: 0.838 | dev_uas: 0.767 |\n","Epoch:  26 | avg_train_loss: 0.836 | dev_uas: 0.763 |\n","Epoch:  27 | avg_train_loss: 0.836 | dev_uas: 0.766 |\n","Epoch:  28 | avg_train_loss: 0.836 | dev_uas: 0.765 |\n","Epoch:  29 | avg_train_loss: 0.834 | dev_uas: 0.767 |\n"]}],"source":["# set the device (GPU if available) and print it as information\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device available:\", device)\n","\n","# initialize the model\n","model_base = Net(device)\n","model_base = model_base.to(device)\n","\n","# define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model_base.parameters(), lr=LR)\n","\n","# for each epoch\n","for epoch in range(EPOCHS):\n","    # perform a training step end evaluate the model\n","    avg_train_loss = train(model_base, train_dataloader_base, criterion, optimizer)\n","    val_uas = test_base(model_base, validation_dataloader_base)\n","\n","    # print loss and score information for each epoch\n","    print(\n","        \"Epoch: {:3d} | avg_train_loss: {:5.3f} | dev_uas: {:5.3f} |\".format(\n","            epoch, avg_train_loss, val_uas\n","        )\n","    )"]},{"cell_type":"markdown","source":["# BERT-based model"],"metadata":{"id":"3JeUkvzYuRo9"}},{"cell_type":"markdown","source":["In the following cells the preprocessing, definition and training steps for the BERT-based model are performed.\n","\n"],"metadata":{"id":"3FdWMeMxuUJX"}},{"cell_type":"markdown","metadata":{"id":"FHTh6Da4qSe_"},"source":["## BERT model - PREPROCESSING"]},{"cell_type":"markdown","metadata":{"id":"U3gUMwJtqWar"},"source":["In this section some utils for preprocessing the dataset for the BERT model are defined. The functions are very similar to the ones defined for the biLSTM-based model, however not identical due to the different information that is required for the BERT-based model."]},{"cell_type":"markdown","metadata":{"id":"X2GsNO27q2G1"},"source":["### Preprocessing utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qWlysPwqWAV"},"outputs":[],"source":["# Extracts the needed information from a sample of a dataset.\n","# The input is a sample of our dataset.\n","def process_sample_bert(sample, get_gold_path=False):\n","    # add the root token to the sentence and its head (-1) to the gold head list\n","    sentence = [\"<ROOT>\"] + sample[\"tokens\"]\n","    gold = [-1] + [int(i) for i in sample[\"head\"]]\n","\n","    # save the tokens\n","    sample_tokens = []\n","    sample_tokens.append(\"<ROOT>\")\n","    for token in sample[\"tokens\"]:\n","        sample_tokens.append(token)\n","\n","    # gold_configurations and gold_transitions are parallel arrays whose elements refer to parsing steps\n","    # gold_configurations[i] records configuration at step i, i.e. topmost stack token and first buffer token for current step\n","    gold_configurations = []\n","    # gold_transitions[i] contains oracle (canonical) transition for step i: 0 is left_arc, 1 right_arc, 2 reduce, 3 shift\n","    gold_transitions = []\n","\n","    # only for training\n","    if get_gold_path:\n","        parser = ArcEager(sentence)\n","        oracle = Oracle(parser, gold)\n","\n","        while not parser.is_tree_final():\n","            # save configuration\n","            configuration = [parser.stack[-1]]\n","            if len(parser.buffer) == 0:\n","                configuration.append(-1)\n","            else:\n","                configuration.append(parser.buffer[0])\n","            gold_configurations.append(configuration)\n","\n","            # save gold transition\n","            if oracle.is_left_arc_gold():\n","                gold_transitions.append(0)\n","                oracle.parser.left_arc()\n","            elif oracle.is_right_arc_gold():\n","                gold_transitions.append(1)\n","                oracle.parser.right_arc()\n","            elif oracle.is_reduce_gold():\n","                gold_transitions.append(2)\n","                oracle.parser.reduce()\n","            else:\n","                gold_transitions.append(3)\n","                oracle.parser.shift()\n","\n","    # sample_tokens is a list containing the tokens in sample\n","    # gold is a list containning the representation of the gold tree of sample\n","    # gold_configurations is a list containing gold configurations\n","    # gold_transitions is a list containing gold transitions\n","    return sample_tokens, gold_configurations, gold_transitions, gold\n","\n","\n","# This function is used to pre-process a batch of samples batch_data from the original dataset,\n","# applying the function process_sample to each sample in batch_data.\n","def prepare_batch_bert(batch_data, get_gold_path=False):\n","    processed_batch_data = [\n","        process_sample_bert(sample, get_gold_path=get_gold_path)\n","        for sample in batch_data\n","    ]\n","\n","    samples_tokens = []\n","    paths = []\n","    moves = []\n","    gold_trees = []\n","\n","    for sample in processed_batch_data:\n","        samples_tokens.append(sample[0])\n","        paths.append(sample[1])\n","        moves.append(sample[2])\n","        gold_trees.append(sample[3])\n","\n","    # samples_tokens, paths, moves, gold_trees are parallel lists\n","    # element in position i of each of the above lists refers to the same sentence i\n","    return samples_tokens, paths, moves, gold_trees"]},{"cell_type":"markdown","metadata":{"id":"FVOOQPcPrVz6"},"source":["### Data loaders"]},{"cell_type":"markdown","metadata":{"id":"lOpevWiBrldS"},"source":["We need to load the sets again, preparing batches according to the function $prepare\\_batch\\_bert$.\n","In this way, we will be able to train our model in parallel in the samples belonging to the same batch and to do inference in the same way."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_O5boUBtrfR-"},"outputs":[],"source":["# set the number of samples per batch\n","BATCH_SIZE = 32\n","\n","# create dataloaders to batch each dataset and apply function prepare_batch to each batch\n","train_dataloader_bert = torch.utils.data.DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    collate_fn=partial(prepare_batch_bert, get_gold_path=True),\n",")\n","validation_dataloader_bert = torch.utils.data.DataLoader(\n","    dev_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    collate_fn=partial(prepare_batch_bert),\n",")\n","test_dataloader_bert = torch.utils.data.DataLoader(\n","    test_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    collate_fn=partial(prepare_batch_bert),\n",")"]},{"cell_type":"markdown","metadata":{"id":"F0j7IiH7HBz-"},"source":["## BERT model - DEFINITION\n"]},{"cell_type":"markdown","metadata":{"id":"irGTSDIN9rtW"},"source":["Install the required libraries."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0Ax-iTbZR03","outputId":"c59b6723-f035-439d-d67e-3a5ae7fd5d79"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, transformers\n","Successfully installed safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.13.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.22.4)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.4.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.15.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n","Installing collected packages: responses, evaluate\n","Successfully installed evaluate-0.4.0 responses-0.18.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting accelerate\n","  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.20.3\n"]}],"source":["# install some other required components\n","!pip install transformers\n","!pip install evaluate\n","!pip install accelerate"]},{"cell_type":"markdown","source":["### Description of the BERT-based model"],"metadata":{"id":"fzHWg_qhu7Cb"}},{"cell_type":"markdown","source":["Here we create the class $CustomBertBasedModel$, which implements the BERT-based model.\n","\n","The model architecture is composed by two main blocks:\n","* a pre-trained encoder-only BERT model to extract contextualized embeddings from tokens;\n","* a feedforward neural network that works as a classifier with the four possible transitions as labels.\n","\n","To better understand the model, let us briefly explain what happens during training to an input batch:\n","* the $bert-base-uncased$ model, which is basically an encoder-only BERT, consists of a transformer. As a first thing, we use BERT tokenizer to tokenize the sentences in the batch that has to be processed in parallel. Then, we can feed the BERT encoder to extract a feature vector of size `768` for each token of each sentence. Since $bert-base-uncased$ is a pre-trained model, we know that such vectors will embed tokens in a contextualized way. We also know that, thanks to the transformer structure of the encoder, the contextualized embeddings of tokens will be much more effective than the ones provided by the BiLSTM. Therefore, we expect our embeddings to be very significative.\n","* Average layer: now we have embeddings for the tokens provided by the BERT tokenizer, but actually we would like to have embeddings for the true tokens (full words) already included in each sample of our batch. In order to do this, for each true token:\n","    * if the desired true token and the current BERT token are the same, then we assign the embedding of the BERT token to the desired one;\n","    * otherwise, we iterate through the subtokens of the desired token that have been produced by the BERT tokenizer and we assign to the true token the mean of its BERT subtokens.\n","* At this point, for each sentence in the batch, we look at all the gold configurations extracted during the pre-processing phase. Each configuration consists of two tokens, so we take the embeddings for those tokens and we concatenate them in order to have a feature vector of size `2 * 768` representing the configuration. Once having so built the representations for all configurations of all sentences in the batch, we can feed a feedforward neural network that classifies those configurations. The input size of the first layer of the FNN has to be `2 * 768`, while the output size of the last layer is equal to the number of labels (i. e. the number of transitions that the parser can do) `4`."],"metadata":{"id":"lYlZQhkWu-H5"}},{"cell_type":"markdown","metadata":{"id":"L6TP3MCOc3BN"},"source":["### Definition of the BERT-based model\n","The class implementing the custom BERT-based model follows."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCtyXzlt_Po_"},"outputs":[],"source":["# Model design parameters\n","BERT_TOKEN_EMBEDDING_SIZE = 768\n","CLASSIFICATION_LABELS = 4\n","WORD_PER_CONFIGURATION = 2\n","# Model hyperparameters\n","LINEAR_LAYER_SIZE = 384\n","DROPOUT = 0.2\n","# Leagning rates and epochs\n","EPOCHS_BERT_FREEZED = 2\n","EPOCHS_BERT_TRAINED = 15\n","LEARNING_RATE_BERT = 1e-4\n","LEARNING_RATE_FNN = 1e-3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9ZrMNmvvyR-"},"outputs":[],"source":["from transformers import BertModel\n","from transformers import BertTokenizer\n","\n","\n","# Custom Bert model\n","class CustomBertBasedModel(nn.Module):\n","    # CONSTRUCTORS\n","\n","    # Constructor specifications\n","    def __init__(self, device):\n","        # inherit the constructor of nn.Module\n","        super(CustomBertBasedModel, self).__init__()\n","\n","        # name of the Bert model to be used\n","        bert_model_name = \"bert-base-uncased\"\n","\n","        # tokenizer used for Bert\n","        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n","\n","        # Bert model\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","\n","        # feedforward head for classification\n","        self.dropout = nn.Dropout(DROPOUT)\n","        self.linear1 = nn.Linear(\n","            BERT_TOKEN_EMBEDDING_SIZE * WORD_PER_CONFIGURATION,\n","            LINEAR_LAYER_SIZE,\n","            bias=True,\n","        )\n","        self.activation1 = nn.Tanh()\n","\n","        self.linear2 = nn.Linear(LINEAR_LAYER_SIZE, CLASSIFICATION_LABELS, bias=True)\n","        self.softmax = nn.Softmax(\n","            dim=-1\n","        )  # the dimension where the softmax has to be applied is the last one\n","\n","        # device\n","        self.device = device\n","\n","    # UTILITY FUNCTIONS: ARCEAGER PARSE\n","\n","    # Check if all parsers $parsers have finished parsing their sentences.\n","    # Returns:\n","    #   ~ True if all parsers have finished;\n","    #   ~ False otherwise.\n","    def parsed_all(self, parsers):\n","        for parser in parsers:\n","            if not parser.is_tree_final():\n","                return False\n","        return True\n","\n","    # The function provides a the next configuration for each parser in parsers.\n","    def get_configurations(self, parsers):\n","        # list of current configurations: configurations[i] is the current configuration for parser parsers[i]\n","        configurations = []\n","\n","        for parser in parsers:\n","            # current configuration of parser\n","            configuration = []\n","\n","            if parser.is_tree_final():\n","                # empty configuration\n","                configuration = [-1, -1]\n","\n","            else:\n","                # append the first element of the configuration: the first element on top of the stack\n","                configuration.append(parser.stack[-1])\n","                if len(parser.buffer) == 0:\n","                    # no element in the buffer, so append $-1\n","                    configuration.append(-1)\n","                else:\n","                    # append the second element of the configuration: the first element in the buffer\n","                    configuration.append(parser.buffer[0])\n","\n","            # append the current configuration for the current parser parser\n","            # we add [] in order to append a list with only one configuration: necessary for prepare_input_fnn\n","            configurations.append([configuration])\n","\n","        return configurations\n","\n","    # Function that selects and performs the next ArcEager transition for each sentence $i in the batch,\n","    # according to the scores in fnn_output.\n","    # parsers is a list of ArcEager parsers: parsers[i] is parsing sentence i in the batch.\n","    # fnn_output is the output tensor of the fnn predicting scores for the CLASSIFICATION_LABELS possible transitions.\n","    # fnn_output has size (# sentences in batch, CLASSIFICATION_LABELS).\n","    def parse_step(self, parsers, fnn_output):\n","        # get the indices of the top 3 transition scores\n","        # top_k_transitions_indices[i] contains the indices in fnn_output[i] (i.e. for sentence i) sorted by predicted transition score\n","        top_k_transitions_indices = torch.topk(fnn_output, 4, dim=-1).indices\n","        # transitions with highest score: best_transitions[i] is the transition with highest score for sentence i\n","        best_transitions = top_k_transitions_indices[:, 0]\n","        second_best_transitions = top_k_transitions_indices[:, 1]\n","        third_best_transitions = top_k_transitions_indices[:, 2]\n","\n","        # get the best transition for each parser in parsers\n","        for i in range(len(parsers)):\n","            # do nothing if the parser parsers[i] has already parsed the whole sentence\n","            if parsers[i].is_tree_final():\n","                continue\n","\n","            else:\n","                # transition with highest probability: left-arc\n","                if best_transitions[i] == 0:\n","                    # note that the buffer is never empty for sure\n","                    # check that sigma_1 is not the root and sigma_1 doeas not already have a head\n","                    if (\n","                        parsers[i].stack[-1] != 0\n","                        and parsers[i].arcs[parsers[i].stack[-1]] == -1\n","                    ):\n","                        parsers[i].left_arc()\n","\n","                    # if the preconditions for the left-arc are not satisfied, I can do right-arc, reduce or shift without any problem,\n","                    # so pick the transition with highest probability\n","                    else:\n","                        # the second best transition is right-arc\n","                        if second_best_transitions[i] == 1:\n","                            parsers[i].right_arc()\n","\n","                        # the second best transition is reduce\n","                        elif second_best_transitions[i] == 2:\n","                            # if sigma_1 is not the root, then we can reduce, because for sure sigma_1 already has a head\n","                            # this is because only one of the two preconditions of left-arc is not satisfied (never both)\n","                            if parsers[i].stack[-1] != 0:\n","                                parsers[i].reduce()\n","\n","                            # sigma_1 is the root: we cannot do a reduce\n","                            # at this point we can do only right-arc or shift: no problems for both of them\n","                            else:\n","                                # right-arc has higher score\n","                                if third_best_transitions[i] == 1:\n","                                    parsers[i].right_arc()\n","                                # shift has higher score\n","                                else:\n","                                    parsers[i].shift()\n","\n","                        # if the second best transition is shift, then do it without any problem\n","                        else:\n","                            parsers[i].shift()\n","\n","                # transition with highest probability: right-arc\n","                elif best_transitions[i] == 1:\n","                    parsers[i].right_arc()\n","\n","                # transition with highest probability: reduce\n","                elif best_transitions[i] == 2:\n","                    # if sigma_1 is not the ROOT, then we can do reduce\n","                    if parsers[i].stack[-1] != 0:\n","                        parsers[i].reduce()\n","\n","                    # sigma_1 is the root, so we cannot do reduce: we can do right-arc or shift\n","                    else:\n","                        # the second best transition is right-arc\n","                        if second_best_transitions[i] == 1:\n","                            parsers[i].right_arc()\n","\n","                        # the second best transition is shift\n","                        else:\n","                            parsers[i].shift()\n","\n","                # transition with highest probability: shift\n","                else:\n","                    # we can do the shift without any problem\n","                    parsers[i].shift()\n","\n","    # UTILITY FUNCTIONS: MODEL\n","\n","    # Given a list of tokens list_tokens representing words in a sentence, the function reconstructs the\n","    # string representing the sentence.\n","    def sentence_reconstruction(self, list_tokens):\n","        sentence = \"\"\n","        for token in list_tokens:\n","            sentence = sentence + \" \" + token\n","        return sentence\n","\n","    # The function averages tokens hidden representations hidden_vectors in order to\n","    # obtain a representation for each token in desired_tokens starting from tokens in current_tokens.\n","    # More precisely, it averages representations of tokens in current_tokens that are subtokens of the\n","    # same token in desired_tokens.\n","    # hidden_vectors contains representations for tokens in current_tokens. It is a torch tensor of size\n","    # (max sentence length in batch, $BERT_TOKEN_EMBEDDING_SIZE).\n","    # current_tokens contains the tokens for which we have a corresponding representation in hidden_vectors.\n","    # desired_tokens contains the tokens for which we want a representation.\n","    # returns the tensor torch.stack(new_hidden_vectors, dim=0) of size (len(desired_tokens), BERT_TOKEN_EMBEDDING_SIZE), where\n","    # torch.stack(new_hidden_vectors, dim=0)[i] is the hidden vector representing desired_tokens[i] as the mean\n","    # of the hidden vectors in hidden_vectors representing tokens in current_tokens that are subtokens of desired_tokens[i].\n","    def average_layer(self, hidden_vectors, current_tokens, desired_tokens):\n","        # list of hidden vectors representing desired_tokens. Actually, it is a list of torch tensors.\n","        new_hidden_vectors = []\n","\n","        # index of the current token in current_tokens\n","        current_token_index = -1\n","\n","        # build one hidden vector per token in desired_tokens\n","        for word in desired_tokens:\n","            # update the index\n","            current_token_index = current_token_index + 1\n","\n","            # current subword of word\n","            current_word = current_tokens[current_token_index]\n","\n","            # list of hidden vectors in hidden_vectors representing tokens that are subword tokens of word\n","            sub_word_vectors = [hidden_vectors[current_token_index]]\n","\n","            # append hidden vectors representing subword tokens of $word until word and current_word are equal\n","            while not len(current_word) == len(word):\n","                # we add to current_word the next subword token of word\n","                current_token_index = current_token_index + 1\n","                if current_tokens[current_token_index].startswith(\"##\"):\n","                    current_word = (\n","                        current_word + current_tokens[current_token_index][2:]\n","                    )\n","                else:\n","                    current_word = current_word + current_tokens[current_token_index]\n","                sub_word_vectors.append(hidden_vectors[current_token_index])\n","\n","            # build a tensor of size (len(sub_word_vectors), BERT_TOKEN_EMBEDDING_SIZE) from the list $sub_word_vectors by stacking subword\n","            # vectors along the first dimension\n","            stacked_tensor = torch.stack(sub_word_vectors, dim=0)\n","\n","            # append to the list new_hidden_vectors the hidden vector representing word, which is computed as the mean\n","            # of the hidden vectors in hidden_vectors representing its subword tokens\n","            new_hidden_vectors.append(torch.mean(stacked_tensor, dim=0))\n","\n","        # we use torch.stack to build a tensor of size (len(desired_tokens), BERT_TOKEN_EMBEDDING_SIZE) from the\n","        # list of tensors new_hidden_vectors\n","        return torch.stack(new_hidden_vectors, dim=0)\n","\n","    # The function, given the outputs of Bert applied to a batch of sentences, returns the inputs for the FNN.\n","    # bert_output contains Bert representation of the batch of sentences.\n","    # bert_output.last_hidden_state is a tensor of size (# sentences in batch, # tokens in max sentence length, BERT_TOKEN_EMBEDDING_SIZE)\n","    # sentences_words[i] is the list of words in sentence i in the batch.\n","    # sentences_bert_tokens[i] is the list of tokens provided by Bert tokenizer applied to sentence i in the batch.\n","    # configurations[i] contains the configurations of sentence i in the batch.\n","    # returns fnn_input: it is a tensor of features representing configurations. Its size is (# configurations, 2 * BERT_TOKEN_EMBEDDING_SIZE).\n","    def prepare_input_fnn(self, bert_output, sentences_words, sentences_bert_tokens, configurations):\n","        # fnn_input[i] will contain the input for the FNN related to sample i in the batch\n","        fnn_input = []\n","\n","        # iterate over the number of samples in the batch, i.e. the number of sentences processed by Bert\n","        for i in range(len(sentences_words)):\n","            # embedding representation of current sentence provided by Bert\n","            # hidden_repr_sentence is a tensor of size (# tokens in the sentence, BERT_TOKEN_EMBEDDING_SIZE)\n","            hidden_repr_sentence = bert_output.last_hidden_state[i]\n","            # average embeddings of tokens belonging to the same word\n","            words_bert_embeddings = self.average_layer(\n","                hidden_repr_sentence, sentences_bert_tokens[i], sentences_words[i]\n","            )\n","\n","            # iterate over the configurations of the current sentence and append the representation of each configuration to fnn_inputs\n","            for configuration in configurations[i]:\n","                # if configuration[0] == -1, then we have no word on the stack, so the representation for the token on the stack is zero\n","                tensor_sigma = torch.zeros(\n","                    BERT_TOKEN_EMBEDDING_SIZE, requires_grad=False\n","                ).to(self.device)\n","                # else, if there is something on top of the stack, then we take its Bert representation words_bert_embeddings[configuration[0]]\n","                if configuration[0] != -1:\n","                    tensor_sigma = words_bert_embeddings[configuration[0]].to(\n","                        self.device\n","                    )\n","\n","                # if configuration[1] == -1, then we have no word in the buffer, so the representation for the token in the stack is zero\n","                tensor_beta = torch.zeros(\n","                    BERT_TOKEN_EMBEDDING_SIZE, requires_grad=False\n","                ).to(self.device)\n","                # else, if there is something at the beginning of the buffer, then its Bert representation is\n","                # words_bert_embeddings[configuration[1]]\n","                if configuration[1] != -1:\n","                    tensor_beta = words_bert_embeddings[configuration[1]].to(\n","                        self.device\n","                    )\n","\n","                # we concatenate the representation of the current configuration configuration as the concatenation of the embeddings\n","                # of, respectively, the first token on top of the stack and the first token in the buffer\n","                fnn_input.append(torch.cat([tensor_sigma, tensor_beta]))\n","\n","        # the input for the fnn is the representations of all configurations of all sentences in the batch\n","        # we stack them into  single tensor\n","        fnn_input = torch.stack(fnn_input).to(self.device)\n","\n","        return fnn_input\n","\n","    # For each list of tokens in sentences_tokens, the function reconstructs the string representing the\n","    # original sentence. Moreover, it applies the Bert tokenizer to the batch of reconstructed sentences and prepares the\n","    # batch for being processed as input of the Bert model.\n","    # Returns:\n","    #   ~ bert_input: input for Bert model;\n","    #   ~ sentences_tokenizers: list containing the list of tokens returned by Bert tokenizer for each sentence.\n","    def bert_tokenization_and_input(self, sentences_tokens):\n","        # list containing all sentences in the batch\n","        sentences = []\n","\n","        # iterate over the samples in the batch\n","        for sentence_tokens in sentences_tokens:\n","            # reconstruct the original sentence string by the word tokens sentence_tokens\n","            sentences.append(self.sentence_reconstruction(sentence_tokens))\n","\n","        # get the input representation to feed Bert with the batch of sentences sentences\n","        bert_input = self.bert_tokenizer(\n","            sentences,\n","            padding=True,\n","            truncation=True,\n","            add_special_tokens=False,\n","            return_tensors=\"pt\",\n","        )\n","\n","        # sentence_tokenizers[i] contains tokens provided by Bert tokenizer of sentence i\n","        sentences_tokenizers = []\n","        # get the ids of the tokens of the sentences in the batch\n","        input_ids = bert_input[\"input_ids\"]\n","        # Convert each row in the input_ids tensor to a list of tokens\n","        for row in input_ids:\n","            sentences_tokenizers.append(self.bert_tokenizer.convert_ids_to_tokens(row))\n","\n","        return bert_input, sentences_tokenizers\n","\n","    # IMPORTANT INSTANCE FUNCTIONS\n","\n","    # Function that applies the feedforward neural network to the input tensor $input.\n","    # It returns the output tensor of the fnn.\n","    def fnn(self, input):\n","        # first layer of the fnn\n","        output_layer1 = self.activation1(self.linear1(self.dropout(input)))\n","        # second layer of the fnn\n","        output_layer2 = self.softmax(self.linear2(self.dropout(output_layer1)))\n","\n","        return output_layer2\n","\n","    # Forward method that we use at training time. It does the forward pass for a batch of samples at the time.\n","    # sentences_tokens[i] is the list of tokens of sentence i in the batch.\n","    # configurations contains the configurations of all samples in the batch.\n","    # configurations[i] is the list of configurations related to sentences[i].\n","    # configurations[i][i] is the configuration at step $i when parsing sentence sentences[i].\n","    def forward(self, sentences_tokens, configurations):\n","        # bert_input contains the input for the bert model, after having applied self.bert_tokenizer the batch\n","        # bert_tokenized_sentences is a list containing tokenized sentences of the batch.\n","        bert_input, bert_tokenized_sentences = self.bert_tokenization_and_input(\n","            sentences_tokens\n","        )\n","        bert_input.to(self.device)\n","\n","        # get the Bert output\n","        bert_output = self.bert(**bert_input)\n","\n","        # prepare the inputs for the feedforward neural network\n","        fnn_input = self.prepare_input_fnn(\n","            bert_output, sentences_tokens, bert_tokenized_sentences, configurations\n","        )\n","\n","        # run the feedforward neural network with all parsing configurations of all sentences in the batch as input\n","        fnn_output = self.fnn(fnn_input)\n","\n","        # return the output of the forward pass in the whole architecture\n","        return fnn_output\n","\n","    # Forward method that we use at inference time.\n","    # It processes a batch of samples at the time.\n","    # It infers a list of parsing trees: one per sample in the batch.\n","    def infere(self, sentences_tokens):\n","        # construct a parser for each sentence in the batch\n","        parsers = [ArcEager(sentence_tokens) for sentence_tokens in sentences_tokens]\n","\n","        # this part is the same as the first part of the $forward function\n","        bert_input, bert_tokenized_sentences = self.bert_tokenization_and_input(\n","            sentences_tokens\n","        )\n","        bert_input.to(self.device)\n","        bert_output = self.bert(**bert_input)\n","\n","        # now we do not have the gold configurations, so we need to compute and score them step by step according to the model's predictions\n","        while not self.parsed_all(parsers):\n","            # get the current configurations of the parsers: configurations[i] is the current configuration of parser parsers[i]\n","            configurations = self.get_configurations(parsers)\n","\n","            fnn_input = self.prepare_input_fnn(\n","                bert_output, sentences_tokens, bert_tokenized_sentences, configurations\n","            )\n","            fnn_output = self.fnn(fnn_input)\n","\n","            # given the current configuration, select the correct transition based on the scores given by the fnn\n","            self.parse_step(parsers, fnn_output)\n","\n","        # return the predicted dependency trees: one per sentence in the batch\n","        return [parser.arcs for parser in parsers]"]},{"cell_type":"markdown","metadata":{"id":"zTrtlc7TsOhF"},"source":["## BERT model - TRAINING\n"]},{"cell_type":"markdown","metadata":{"id":"SuF4lyl5sbqF"},"source":["Some considerations:\n","- training the whole model (both pre-trained Bert and classification head) is considered fine-tuning, because we are fine-tuning weights to better suit our task;\n","- training the whole model directly could cause catastrophic forgetting: since the classification head has weights that are randomly initialized, it could cause a sensible change in the weights of the Bert model, causing loss of pre-trained knowledge. Hence, a good idea could be to divide the training into two phases:\n","    - keep Bert pre-trained weights frozen and update only the classification head's ones. In this way, we make the classification head adapt to the pre-trained weights of Bert (as well as to our specific task). Just one or two epochs are sufficient;\n","    - do a second training by updating also Bert weights. The risk of catastrophic forgetting is now mitigated by the fact that in the previous training phase, the weights of the classification head have adapted to Bert ones and are no more random.\n","- We have tried to change the number of epochs, the size of the classification head and many other parameters, but the ones reported are those that reported the better trade-off results-consistency."]},{"cell_type":"markdown","metadata":{"id":"mG7FR8yO2MlO"},"source":["We decided to use a class for agglomerating everything needed for training the BERT-based model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMY-hPHv116l"},"outputs":[],"source":["# Class for training a model in a custom way defined inside the class itself.\n","class CustomTrain:\n","    # Constructor function.\n","    # model is the model to be trained.\n","    # device is the device to be used for training the model.\n","    def __init__(\n","        self,\n","        model,\n","        device,\n","        epochs=EPOCHS,\n","        loss_function=nn.CrossEntropyLoss(),\n","        bert_freezed=False,\n","    ):\n","        # set the model\n","        self.model = model\n","\n","        # set the loss function: cross entropy loss is the default\n","        self.loss = loss_function\n","\n","        # set the deviced to be used for training the model\n","        self.device = device\n","\n","        # set the number of epochs for the training\n","        self.number_of_epochs = epochs\n","\n","        # set the learning rates\n","        self.learning_rate_bert = LEARNING_RATE_BERT\n","        self.learning_rate_fnn = LEARNING_RATE_FNN\n","\n","        # set the optimizer: Adam\n","        if bert_freezed:\n","            # the weights of the Bert moel are not updated during training\n","            self.optimizer = torch.optim.Adam(\n","                [\n","                    {\"params\": model.linear1.parameters(), \"lr\": LEARNING_RATE_FNN},\n","                    {\"params\": model.linear2.parameters(), \"lr\": LEARNING_RATE_FNN},\n","                ]\n","            )\n","\n","        else:\n","            # the weights of the Bert moel are updated during training\n","            self.optimizer = torch.optim.Adam(\n","                [\n","                    {\"params\": model.bert.parameters(), \"lr\": LEARNING_RATE_BERT},\n","                    {\"params\": model.linear1.parameters(), \"lr\": LEARNING_RATE_FNN},\n","                    {\"params\": model.linear2.parameters(), \"lr\": LEARNING_RATE_FNN},\n","                ]\n","            )\n","\n","    # Function to train model on dataloader based on the loss function loss_function.\n","    # It does only one epoch.\n","    # model is the model to be trained.\n","    # dataloader is the dataloader containing the preprocessed batches.\n","    # loss_function is the loss function to be used for error and gradient computation.\n","    # optimizer is the optimizer to be used to update the learnable parameters according to\n","    # the gradient of loss_function.\n","    def single_epoch_train(self, dataloader):\n","        # set the model to train phase\n","        self.model.train()\n","\n","        # initial loss value\n","        total_loss = 0\n","        # initial number of batches used for training\n","        count_batches = 0\n","\n","        # iterate over the batches in the dataset\n","        for batch in dataloader:\n","            # zero the gradients: we are at the beginning of a new batch\n","            self.optimizer.zero_grad()\n","\n","            # load the current batch $batch\n","            samples_tokens, configurations, transitions, gold_trees = batch\n","\n","            # forward step of the model applied to $batch\n","            model_output = self.model(samples_tokens, configurations)\n","\n","            # sum(moves, []) flattens the list of lists $transitions into a single list where\n","            # the single transitions[i] are concatenated\n","            labels = torch.tensor(sum(transitions, [])).to(self.device)\n","\n","            # compute the loss of the current batch $batch w.r.t. the gold labels $labels\n","            loss = self.loss(model_output, labels)\n","\n","            # update the number of batches for which the loss has been computed\n","            count_batches += 1\n","            # update the total loss, i.e. the sum of the losses of the single considered batches\n","            # by adding the loss of the current batch batch\n","            total_loss += loss.item()\n","\n","            # backpropagation step to compute the gradient of the loss w.r.t. the learnable parameters of the model\n","            loss.backward()\n","\n","            # optimization of the learnable parameters of the model\n","            self.optimizer.step()\n","\n","        # return the mean batch loss\n","        return total_loss / count_batches\n","\n","    # Trains the model on the training set provided by $train_dataloader for the given number of epochs.\n","    # train_dataloader is the dataloader providing the batched training set.\n","    # validation_dataloader is the dataloader providing the batched validation set.\n","    # loss_function is the loss function to be used for training the model.\n","    # optimizer is the optimizer to be used to train the model.\n","    # After the application of this function, the models' parameters are updated according to the training procedure.\n","    # Moreover, the function prints the average training loss and the performances of the models on the validation set\n","    # at each epoch.\n","    def custom_train(self, train_dataloader, validation_dataloader):\n","        # iterate over the desired number of epochs\n","        for epoch in range(self.number_of_epochs):\n","            # single training step\n","            average_training_loss = self.single_epoch_train(train_dataloader)\n","\n","            # current performances of the model on the validation set\n","            validation_performances = self.performance_evaluation(validation_dataloader)\n","\n","            # print both average batch loss and validation performances w.r.t. the current training epoch\n","            print(\n","                \"Epoch: {:3d} | Average batch loss training set: {:5.3f} | UAS validation set: {:5.3f} |\".format(\n","                    epoch, average_training_loss, validation_performances\n","                )\n","            )\n","\n","    # Unlabeled accuracy score: percentage of correctly predicted arcs w.r.t. the total number of arcs.\n","    # This evaluation metric is computed for all samples in a batch.\n","    # gold_arcs_batch[i] is the list of gold arcs for sample i in the batch.\n","    # predicted_arcs_batch[i] is the list of predicted arcs for sample i in the batch.\n","    # The function returns the ratio (# correctly predicted arcs in the batch / total # gold arcs in the batch).\n","    def unlabeled_accuracy_score(self, gold_arcs_batch, predicted_arcs_batch):\n","        # total number of checked arcs: initialized to 0\n","        total_number_of_arcs = 0\n","        # number of correctly predicted arcs: initialized to 0\n","        correctly_predicted_arcs = 0\n","\n","        # iterate over the samples in the batch\n","        for gold_arcs_sample, predicted_arcs_sample in zip(\n","            gold_arcs_batch, predicted_arcs_batch\n","        ):\n","            # iterate over the gold arcs of each sample\n","            # we start from 1 because the root is never dependent of any head\n","            for i in range(1, len(gold_arcs_sample)):\n","                total_number_of_arcs += 1\n","\n","                # if the current gold arc of the current sample is equal to the current predicted arc of the current sample,\n","                # then update the number of correctly predicted arcs\n","                if gold_arcs_sample[i] == predicted_arcs_sample[i]:\n","                    correctly_predicted_arcs += 1\n","\n","        return correctly_predicted_arcs / total_number_of_arcs\n","\n","    # Function that computes a measure of the performances of the models in the evaluation set\n","    # provided by dataloader.\n","    # The function used to compute performances values is unlabeled_accuracy_score.\n","    # Returns unlabeled_accuracy_score computed over dataloader.\n","    def performance_evaluation(self, dataloader):\n","        # set the model to evaluation time\n","        self.model.eval()\n","\n","        # list of gold arcs: initialized as empty\n","        gold_arcs_list = []\n","        # list of predicted arcs: initialized as empty\n","        predicted_arcs_list = []\n","\n","        # iterate over the batches in the loaded dataloader\n","        for batch in dataloader:\n","            # load the current batch $batch\n","            samples_tokens, configurations, transitions, gold_trees = batch\n","\n","            # disable torch gradient computation in order not to waste resources\n","            with torch.no_grad():\n","                # use the model to predict the arcs\n","                predicted_arcs_batch = self.model.infere(samples_tokens)\n","\n","                # update the lists with checked arcs\n","                gold_arcs_list += gold_trees\n","                predicted_arcs_list += predicted_arcs_batch\n","\n","        # return the value returned by the evaluation function defined in the class\n","        return self.unlabeled_accuracy_score(gold_arcs_list, predicted_arcs_list)"]},{"cell_type":"markdown","metadata":{"id":"4GaVlFZxHTrP"},"source":["### Train the BERT-based model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wTmfC_2PHS2g","outputId":"5135db1c-0c87-4da9-c6d7-5920eb6f5eb1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Training for 2 epochs with BERT freezed:\n","Epoch:   0 | Average batch loss training set: 1.074 | UAS validation set : 0.589 |\n","Epoch:   1 | Average batch loss training set: 1.004 | UAS validation set : 0.650 |\n","Now training for 15 epochs with BERT trained:\n","Epoch:   0 | Average batch loss training set: 0.903 | UAS validation set : 0.851 |\n","Epoch:   1 | Average batch loss training set: 0.825 | UAS validation set : 0.877 |\n","Epoch:   2 | Average batch loss training set: 0.807 | UAS validation set : 0.891 |\n","Epoch:   3 | Average batch loss training set: 0.798 | UAS validation set : 0.892 |\n","Epoch:   4 | Average batch loss training set: 0.799 | UAS validation set : 0.889 |\n","Epoch:   5 | Average batch loss training set: 0.795 | UAS validation set : 0.888 |\n","Epoch:   6 | Average batch loss training set: 0.791 | UAS validation set : 0.894 |\n","Epoch:   7 | Average batch loss training set: 0.787 | UAS validation set : 0.889 |\n","Epoch:   8 | Average batch loss training set: 0.787 | UAS validation set : 0.898 |\n","Epoch:   9 | Average batch loss training set: 0.784 | UAS validation set : 0.896 |\n","Epoch:  10 | Average batch loss training set: 0.782 | UAS validation set : 0.898 |\n","Epoch:  11 | Average batch loss training set: 0.782 | UAS validation set : 0.890 |\n","Epoch:  12 | Average batch loss training set: 0.782 | UAS validation set : 0.892 |\n","Epoch:  13 | Average batch loss training set: 0.782 | UAS validation set : 0.892 |\n","Epoch:  14 | Average batch loss training set: 0.783 | UAS validation set : 0.893 |\n"]}],"source":["# set the device to be used for training the model model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","# set the model to be trained\n","model_bert = CustomBertBasedModel(device)\n","# save the model to the desired device\n","model_bert.to(device)\n","\n","# BERT FREEZED\n","bert_freezed = True\n","print(\"Training for\", str(EPOCHS_BERT_FREEZED), \"epochs with BERT freezed:\")\n","# initialize the CustomTrain object to train the model\n","trainer = CustomTrain(model_bert, device, bert_freezed=True, epochs=EPOCHS_BERT_FREEZED)\n","# actual training of the model\n","trainer.custom_train(train_dataloader_bert, validation_dataloader_bert)\n","\n","# BERT UNFREEZED\n","print(\"Now training for\", str(EPOCHS_BERT_TRAINED), \"epochs with BERT trained:\")\n","# initialize the CustomTrain object to train the model\n","trainer = CustomTrain(model_bert, device, epochs=EPOCHS_BERT_TRAINED)\n","# actual training of the model\n","trainer.custom_train(train_dataloader_bert, validation_dataloader_bert)"]},{"cell_type":"markdown","metadata":{"id":"6hdbs4CZyQAb"},"source":["# Evaluation"]},{"cell_type":"markdown","source":["In this section we evaluate the two models on the test set to compare their performances."],"metadata":{"id":"nf_cTwiaT8mm"}},{"cell_type":"markdown","source":["## Baseline Bi-LSTM model evaluation"],"metadata":{"id":"OtjYbDYkUDuR"}},{"cell_type":"code","source":["# evaluate UAS performances of the baseline Bi-LSTM trained model on the test set\n","test_uas = test_base(model_base, test_dataloader_base)\n","print(\"UAS test set for Bi-LSTM baseline model: {:5.3f}\".format(test_uas))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4d9ZsrQUas_","outputId":"cb3d2bab-a161-46f8-afed-573168d2f763"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["UAS test set for Bi-LSTM baseline model: 0.766\n"]}]},{"cell_type":"markdown","source":["## BERT-based model evaluation"],"metadata":{"id":"NR8ocz9LUJqo"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i7z38s9nPu8H","outputId":"2caa9bbb-c8ae-4a0e-c566-c6b5ecae3df7"},"outputs":[{"output_type":"stream","name":"stdout","text":["UAS test set for Bi-LSTM baseline model: 0.894\n"]}],"source":["# evaluate UAS performances of the BERT-based trained model on the test set\n","test_uas = trainer.performance_evaluation(test_dataloader_bert)\n","print(\"UAS test set for Bi-LSTM baseline model: {:5.3f}\".format(test_uas))"]},{"cell_type":"markdown","source":["## Discussion about the results of the two models"],"metadata":{"id":"EmJjl44-XG2-"}},{"cell_type":"markdown","source":["As expected, the BERT-based model is performing better that the baseline Bi-LSTM mode. BERT is a very powerful pretrained large language model and its tokens embeddings contain so much information about the meaning and the context. The pretrained embeddings of BERT come from a huge trainig setup with a huge availability of data with huge computation power. The training of the baseline Bi-LSTM model is much simpler with much less data, therefore it is reasonable that it is performing worse. However, the main reason of such a difference in performances, in our opinion, is due to the fact that the BERT-based model uses a transformer to extract contextualized embeddings from tokens, while the BiLSTM is based on a recurrent strategy that cannot consider a context as wide as a transformer can do. The transformer, indeed, works in parallel and uses cross-attention and self-attention to be able to record information of a very wide context around the considered token.\n","\n","Regarding the training procedure of the two models, we are sure that there is still room for improvements by changing the hyperparameters and also by changing some auxiliar functions, such as $parse\\_step$. However, testing different parameters requires a lot of time: The ones that are used in this final version of the notebook are those with which we got the best performances."],"metadata":{"id":"RYbrpSCGXMJd"}},{"cell_type":"markdown","metadata":{"id":"omoPRW-ByS0-"},"source":["# SotA discussion"]},{"cell_type":"markdown","source":["To evaluate the overall performances of our model, we need to compare it with the State of the Art approaches for the depenency parsing task on the same Universal Dependencies (UD) dataset. To make a fair comparison, we should compare models that have been trained and evaluated on the exact same data.\n","\n","Unfortunately, looking at the most recent works we could not find any that proposed a model trained only on the dataset we used: _en_lines_ from UD.\n","\n","In [1], we found an interesting comparison among different models trained for the dependency parsing task on the UD dataset. In particular, the results reported correspond to the average of the evaluation metrics on all the treebanks in the UD dataset. The models in [1] are multilingual, therefore trained on a much larger corpus with the goal of performing well on many different languages with very different syntactic structures. In our case, the baseline and BERT-based models have been trained on a single language and fine-tuned to perform the best on it.\n","\n","Moreover, the models reported in [1] are from 2019 or sooner. The UD dataset is getting richer every year, therefore (even if training on multilingual data) it is quite hard to make a fair objective comparison between our models and the baseline ones.\n","\n","However, even if it has no scientific value, it can be interesting to compare our model with some State of the Art approaches. In particular, we compare with Straka et al. [2], that contains the best performing model from [1], where the best achieved UAS is **87.28**. In the case of our baseline Bi-LSTM and BERT-based models, we achieved UAS **76.40** and **89.50** respectively. Please note that these scores are computed as an avarage among 5 different trainings therefore a fresh new training may lead to slightly different results.\n","\n","As expected, the Bi-LSTM model is performing the worst, since both our BERT-based model and the ones in [2] are based on pretrained embeddings from large language models. Interestingly, our BERT-based model is performing the best. This can be explained by the following observations:\n","1. The model in [2] is multilingual, therefore it has to perform well on different languages with very different syntactic structures. Instead, in our case, we worked only on a single language and we fine-tuned our model to perform the best only on english language.\n","2. It might be possible that the version of BERT that we used is more powerful than the one used in [2] from 2019 (first release of BERT) and this may be another reason why we got better results.\n","\n","\n","**References**\n","\n","[1] paperswithcode.com UD dependency parsing UAS leaderboard - https://paperswithcode.com/sota/dependency-parsing-on-universal-dependencies?metric=UAS\n","\n","[2] Milan Straka, Jana Strakovà, Jan Hajic. 2019. Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Parsing."],"metadata":{"id":"78L4i3wNaiEH"}},{"cell_type":"markdown","source":["# Conclusions"],"metadata":{"id":"ytKkgIPrL21s"}},{"cell_type":"markdown","source":["To conclude, the results we got are in line with what we have studied during NLP classes: the model based on BERT is performing better with respect to the baseline one. As we have seen, the process of fine-tuning large language models can lead to very good performances for many different tasks, a process (like in this case) that can lead to much better results with respect to building from scratch a new model, especially when there is no availability for large datasets and enough computational power resources. Of course our experience in tuning properly the hyperparameters of deep learning models is poor, therefore we are sure that there is still much room for improvments in the models that we have reported in the notebook."],"metadata":{"id":"iKmz18gIL5dC"}},{"cell_type":"markdown","source":["Just because we do not want to conclude the project with our very boring considerations detailed in the above sections, to provide something different, we let ChatGPT (https://chat.openai.com/share/7cd18497-bf53-4fd5-8a0e-7f64359c4ca2) conclude:\n","\n","_In conclusion, our university project on NLP dependency parsing proved to be an invaluable learning experience. Through the development of this project, we gained a deep understanding of the course topics, particularly in the area of fine-tuning pretrained models for specific tasks. This hands-on exploration allowed us to acquire practical skills and insights that will undoubtedly benefit our future endeavors in natural language processing._"],"metadata":{"id":"KcXBWK4EOGS5"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"14ba408fd73347c798900adad2cadec0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cfbd3d87ba574d6e8d5b6d6f2b332ace","IPY_MODEL_18e72b79dcc14bc2893f64592c7fd33e","IPY_MODEL_7bf65a7c07ab4113b3fb5655f451ed0d"],"layout":"IPY_MODEL_38482f6f1f4f4d93b7e3bec4789b5800"}},"cfbd3d87ba574d6e8d5b6d6f2b332ace":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a252976311234ff19b043dc0bbb0c4a1","placeholder":"​","style":"IPY_MODEL_8c36251b9f594243bb08c45a7d87041f","value":"Downloading builder script: 100%"}},"18e72b79dcc14bc2893f64592c7fd33e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4242b2725084a259cb9d9ef5942584c","max":87751,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d585c7e01b534a49b8335291c3590f01","value":87751}},"7bf65a7c07ab4113b3fb5655f451ed0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68110a9b63c94b40890d1c614aab9bf3","placeholder":"​","style":"IPY_MODEL_7a191cfc0d7e444d8a88c8a971a5b32c","value":" 87.8k/87.8k [00:00&lt;00:00, 1.06MB/s]"}},"38482f6f1f4f4d93b7e3bec4789b5800":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a252976311234ff19b043dc0bbb0c4a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c36251b9f594243bb08c45a7d87041f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4242b2725084a259cb9d9ef5942584c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d585c7e01b534a49b8335291c3590f01":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68110a9b63c94b40890d1c614aab9bf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a191cfc0d7e444d8a88c8a971a5b32c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f931f5d677584f77a44d8721631492f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe1d8424b41c4587b0b1c3c45725c342","IPY_MODEL_920b8530633a4680a7df2890243c8df6","IPY_MODEL_b57f9384adbc48988c2aeb5af4a8ebd7"],"layout":"IPY_MODEL_29e1e600e46c4be38a2b95fe5b47b281"}},"fe1d8424b41c4587b0b1c3c45725c342":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d0a3a19930f4aafb9544d42c31752fc","placeholder":"​","style":"IPY_MODEL_732b5bad45cb4fbc93ad253f7fc91909","value":"Downloading metadata: 100%"}},"920b8530633a4680a7df2890243c8df6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c13a01c8d2324dfc92eca91dce080fc9","max":2333578,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19c3126ecda7493ca07196bfba1833e2","value":2333578}},"b57f9384adbc48988c2aeb5af4a8ebd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b5969dbc7e74114a8f4a1352b952181","placeholder":"​","style":"IPY_MODEL_bc7c2ee676b24270bc8e95732aff0672","value":" 2.33M/2.33M [00:00&lt;00:00, 6.76MB/s]"}},"29e1e600e46c4be38a2b95fe5b47b281":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d0a3a19930f4aafb9544d42c31752fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"732b5bad45cb4fbc93ad253f7fc91909":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c13a01c8d2324dfc92eca91dce080fc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19c3126ecda7493ca07196bfba1833e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b5969dbc7e74114a8f4a1352b952181":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc7c2ee676b24270bc8e95732aff0672":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"685ddb6b91c543a9995afb32593ddf61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_824992f915b64136ab6326d6b3c99903","IPY_MODEL_a84b750fdb8b4f9e99f1ebf78d54416d","IPY_MODEL_505fdd1839e84b74bbc2eaba57500770"],"layout":"IPY_MODEL_fd4a68745b374009a680acb32d27ed99"}},"824992f915b64136ab6326d6b3c99903":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c733e8de923424b959b463ce29ed709","placeholder":"​","style":"IPY_MODEL_59bfa05904a64e7cac8eb90502640af0","value":"Downloading readme: 100%"}},"a84b750fdb8b4f9e99f1ebf78d54416d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0307f6e57a9146499ca236f190aafeed","max":191193,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ff3ec04ba74432ab24d6f8680b1440b","value":191193}},"505fdd1839e84b74bbc2eaba57500770":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7f78c3100ac4578bc8f32067e0e2af5","placeholder":"​","style":"IPY_MODEL_9ca5273015204a13ae9ba318c52bab4c","value":" 191k/191k [00:00&lt;00:00, 6.40MB/s]"}},"fd4a68745b374009a680acb32d27ed99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c733e8de923424b959b463ce29ed709":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59bfa05904a64e7cac8eb90502640af0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0307f6e57a9146499ca236f190aafeed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ff3ec04ba74432ab24d6f8680b1440b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e7f78c3100ac4578bc8f32067e0e2af5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ca5273015204a13ae9ba318c52bab4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0dbda08363434b55a96616d27100bd93":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_285a0d13048b4d77bae402ba8a2dc563","IPY_MODEL_26438e5ebb98469994a92374fd36aab0","IPY_MODEL_256d29fe5f7144a08eed6d055191e10a"],"layout":"IPY_MODEL_83925a91960b426285c3c1ee8aa063e1"}},"285a0d13048b4d77bae402ba8a2dc563":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19d43521019640a29b9e4c6293c66181","placeholder":"​","style":"IPY_MODEL_eea5ab2cfe1f4965b5c8f836b32d156d","value":"Downloading data files: 100%"}},"26438e5ebb98469994a92374fd36aab0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eda6de9d06a74201b1e84cd4cafa4ecc","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0c6a11c305234d81bf1f75b4491e006b","value":3}},"256d29fe5f7144a08eed6d055191e10a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb09c5f4d53c4310b6c1c8173d8134ca","placeholder":"​","style":"IPY_MODEL_07543e62ae0a499ab78f2f8eb61d4b14","value":" 3/3 [00:01&lt;00:00,  2.23it/s]"}},"83925a91960b426285c3c1ee8aa063e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19d43521019640a29b9e4c6293c66181":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eea5ab2cfe1f4965b5c8f836b32d156d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eda6de9d06a74201b1e84cd4cafa4ecc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c6a11c305234d81bf1f75b4491e006b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb09c5f4d53c4310b6c1c8173d8134ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07543e62ae0a499ab78f2f8eb61d4b14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54dd851a77e049c482c13ad6b3ce9b07":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c560ac77117d4bcb897d74a73fac7ca0","IPY_MODEL_1c8f88849cbe4c66a73c429e74d7b926","IPY_MODEL_e302ba7cd1474db68fb58ad2e017c080"],"layout":"IPY_MODEL_2ba48e2c142743c58d79e18352a485c3"}},"c560ac77117d4bcb897d74a73fac7ca0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5040b07cd9d2499bbaf273b510f11e14","placeholder":"​","style":"IPY_MODEL_83158d426e9a42579aaaaa09edfd3a9a","value":"Downloading data: "}},"1c8f88849cbe4c66a73c429e74d7b926":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_052b0e2983274ac78304a3f2b5cd6b5b","max":580364,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be636712a86741e8b338194088e63c54","value":580364}},"e302ba7cd1474db68fb58ad2e017c080":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14a1d6d68c464e2fb88062e9b7a56053","placeholder":"​","style":"IPY_MODEL_d4390fcaa82f4aaf937aa99efe06723c","value":" 3.36M/? [00:00&lt;00:00, 53.2MB/s]"}},"2ba48e2c142743c58d79e18352a485c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5040b07cd9d2499bbaf273b510f11e14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83158d426e9a42579aaaaa09edfd3a9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"052b0e2983274ac78304a3f2b5cd6b5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be636712a86741e8b338194088e63c54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14a1d6d68c464e2fb88062e9b7a56053":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4390fcaa82f4aaf937aa99efe06723c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8de05240f41430887f13ac382b82530":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_983bac7fdb6649989e87082386c71dd4","IPY_MODEL_a7ae27f9601d43b5bb84c1bafe35c6df","IPY_MODEL_d02f31eb011a4ca1a8866317ae7f5573"],"layout":"IPY_MODEL_498dcca95d6546159edc778bb19cd6e4"}},"983bac7fdb6649989e87082386c71dd4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3767b43f519b4deb9ee52ac9da20e5de","placeholder":"​","style":"IPY_MODEL_98deb10113324a7788a30bd202fd6f6f","value":"Downloading data: "}},"a7ae27f9601d43b5bb84c1bafe35c6df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fc36ebfc1024c27b9a3f30b4279c38e","max":198847,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7205ba93fd78485e97b6f56e574d9372","value":198847}},"d02f31eb011a4ca1a8866317ae7f5573":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38f1022fcf934886ad7559f615a753ed","placeholder":"​","style":"IPY_MODEL_5fd09b4c763d4e529949562476b158e4","value":" 1.12M/? [00:00&lt;00:00, 27.8MB/s]"}},"498dcca95d6546159edc778bb19cd6e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3767b43f519b4deb9ee52ac9da20e5de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98deb10113324a7788a30bd202fd6f6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fc36ebfc1024c27b9a3f30b4279c38e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7205ba93fd78485e97b6f56e574d9372":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"38f1022fcf934886ad7559f615a753ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fd09b4c763d4e529949562476b158e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6761aaad23cd4f24822a398d2bc88305":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_03d8a555b6864475b5c060895400e104","IPY_MODEL_928d1a9b3cf54b7787e97888cfffd569","IPY_MODEL_b660ca28332e42c59bdabb3b9ec2e255"],"layout":"IPY_MODEL_af6596169e4c4344ba8720608d58587a"}},"03d8a555b6864475b5c060895400e104":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8040c7e487d04f1a9d8974c39512942b","placeholder":"​","style":"IPY_MODEL_f0c3cc0c86e2411290d254cdebd8fbc8","value":"Downloading data: "}},"928d1a9b3cf54b7787e97888cfffd569":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f04eec2d3bc40299a2fc8decd40a32c","max":180772,"min":0,"orientation":"horizontal","style":"IPY_MODEL_358140b31ac14062a873c28cb31a61e0","value":180772}},"b660ca28332e42c59bdabb3b9ec2e255":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62a05d75716a4c009e7d94d536338727","placeholder":"​","style":"IPY_MODEL_227c1ca2f18845dbb7acdcada55d93cf","value":" 1.04M/? [00:00&lt;00:00, 26.2MB/s]"}},"af6596169e4c4344ba8720608d58587a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8040c7e487d04f1a9d8974c39512942b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0c3cc0c86e2411290d254cdebd8fbc8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f04eec2d3bc40299a2fc8decd40a32c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"358140b31ac14062a873c28cb31a61e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62a05d75716a4c009e7d94d536338727":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227c1ca2f18845dbb7acdcada55d93cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"693770d6c6e34bb591da8925df166ebd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a7eec0a45904c84aaf6d0d0ae32a877","IPY_MODEL_3928b038fa534debb825879ff57caf16","IPY_MODEL_3756a5dcf5704ba79f2220b6ca700133"],"layout":"IPY_MODEL_56e7c1f2abda4e2e9d3b94308570ac77"}},"5a7eec0a45904c84aaf6d0d0ae32a877":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_600d7f906d764ec082188e015c9064bd","placeholder":"​","style":"IPY_MODEL_9585d158af3b47a88a96547717b26f01","value":"Extracting data files: 100%"}},"3928b038fa534debb825879ff57caf16":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c12ec7a87f794ad2b3c980a5b0fffd24","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9cf9bebd07194ec2aefabb95ce41339e","value":3}},"3756a5dcf5704ba79f2220b6ca700133":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69a52a901c8140eea8555fe96ef437b8","placeholder":"​","style":"IPY_MODEL_99363db37f2041439c18aae8835b6e25","value":" 3/3 [00:00&lt;00:00, 136.83it/s]"}},"56e7c1f2abda4e2e9d3b94308570ac77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"600d7f906d764ec082188e015c9064bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9585d158af3b47a88a96547717b26f01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c12ec7a87f794ad2b3c980a5b0fffd24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cf9bebd07194ec2aefabb95ce41339e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"69a52a901c8140eea8555fe96ef437b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99363db37f2041439c18aae8835b6e25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47dc925aec47429d94b284e0e112f252":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e29c4567d51493a9de39e772cf8a6f8","IPY_MODEL_09eae29952b44137a63746d794cbe822","IPY_MODEL_135f6b19d52d489692833bb6c1e2ef2f"],"layout":"IPY_MODEL_59f527ac29fe44f991f21771f4e43054"}},"3e29c4567d51493a9de39e772cf8a6f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52137e8c09004b09b01076b54f68dd4b","placeholder":"​","style":"IPY_MODEL_f107f7da4ca14a109644a7066d1666da","value":"Generating train split:  94%"}},"09eae29952b44137a63746d794cbe822":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_962ad4e8f8504344aa0e5d116ae486d8","max":3176,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc048901ee5642f9aa2d0f9d626c70d0","value":3176}},"135f6b19d52d489692833bb6c1e2ef2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8dffe19768f4633bd91d6dad299dd4c","placeholder":"​","style":"IPY_MODEL_66b8045f044646478addbe77063659c0","value":" 2980/3176 [00:02&lt;00:00, 2231.68 examples/s]"}},"59f527ac29fe44f991f21771f4e43054":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"52137e8c09004b09b01076b54f68dd4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f107f7da4ca14a109644a7066d1666da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"962ad4e8f8504344aa0e5d116ae486d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc048901ee5642f9aa2d0f9d626c70d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d8dffe19768f4633bd91d6dad299dd4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66b8045f044646478addbe77063659c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a804d137d8e4655b3127811d3493843":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27b050ae62954951a255b6f591f79fd9","IPY_MODEL_cec4cdff289a41c380ec321776ea536f","IPY_MODEL_8d98a759d86845e2a3e51d936cf54f88"],"layout":"IPY_MODEL_2e64f30bba404033bd17963d518576df"}},"27b050ae62954951a255b6f591f79fd9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb5034991fc0402c8a942bf3d28a6c0c","placeholder":"​","style":"IPY_MODEL_2d02ac1b302c46f89aadbe3ca34aaaec","value":"Generating validation split:  93%"}},"cec4cdff289a41c380ec321776ea536f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9d21ad92603471f8704fbacf421fa9c","max":1032,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf8c170c7a0749c5a3ed7c5a2fe30030","value":1032}},"8d98a759d86845e2a3e51d936cf54f88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65e1760ea3f841498823a77047c1149b","placeholder":"​","style":"IPY_MODEL_bf88eb61be0548f989dc8e550b186592","value":" 964/1032 [00:01&lt;00:00, 889.22 examples/s]"}},"2e64f30bba404033bd17963d518576df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"fb5034991fc0402c8a942bf3d28a6c0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d02ac1b302c46f89aadbe3ca34aaaec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9d21ad92603471f8704fbacf421fa9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf8c170c7a0749c5a3ed7c5a2fe30030":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65e1760ea3f841498823a77047c1149b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf88eb61be0548f989dc8e550b186592":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77bbd647de0e4bc9b4f0f93a26b6a1a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_caf8bac41f9648ae9fade863552a44be","IPY_MODEL_2e725f275e9d48c5b49d7fb0217022d4","IPY_MODEL_aef0d36b18884148ba428eebed3ca79b"],"layout":"IPY_MODEL_095e382783f74b5faf87f4bc0bca455a"}},"caf8bac41f9648ae9fade863552a44be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_691884c3d355441b9d87fc3cea340e32","placeholder":"​","style":"IPY_MODEL_6807029f06a544f1bf5f17f2967054b3","value":"Generating test split:  99%"}},"2e725f275e9d48c5b49d7fb0217022d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_a35a24b3fc2a45b89eeb860df05743b1","max":1035,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d73f7e8d4ac1430cad2412b02eae6b98","value":1035}},"aef0d36b18884148ba428eebed3ca79b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba1688bbe13249bb9ec7040ffdfedf37","placeholder":"​","style":"IPY_MODEL_a2c7f1474aeb4c0c89382e45eafca4dd","value":" 1020/1035 [00:02&lt;00:00, 382.70 examples/s]"}},"095e382783f74b5faf87f4bc0bca455a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"691884c3d355441b9d87fc3cea340e32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6807029f06a544f1bf5f17f2967054b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a35a24b3fc2a45b89eeb860df05743b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d73f7e8d4ac1430cad2412b02eae6b98":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba1688bbe13249bb9ec7040ffdfedf37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2c7f1474aeb4c0c89382e45eafca4dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}